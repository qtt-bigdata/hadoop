From 6089766a4774e5f2e7d6823b95a384efdd0b9fb2 Mon Sep 17 00:00:00 2001
From: Xiao Chen <xiao@cloudera.com>
Date: Tue, 18 Oct 2016 23:25:02 -0700
Subject: [PATCH 1948/2848] HDFS-11009. Add a tool to reconstruct block meta
 file from CLI.

(cherry picked from commit bc4a32aea50e86819730312e89315c0244ce64bf)

Conflicts:
	hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java
	hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetUtil.java

(cherry picked from commit ad7d3c4db8bfab007cc2ec1bad3c388fd7144369)
(cherry picked from commit d22a6a8b82504444b86d95930413978bd3396834)

Conflicts:
	hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/impl/DfsClientConf.java
	hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java
	hadoop-hdfs-project/hadoop-hdfs/src/site/markdown/HDFSCommands.md
	hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/tools/TestDebugAdmin.java

Change-Id: Id6dc35130e237806af5d261fdbd3ad8c7864f323
---
 .../java/org/apache/hadoop/hdfs/DFSClient.java     |    4 +-
 .../datanode/fsdataset/impl/FsDatasetImpl.java     |   46 ++++-----
 .../datanode/fsdataset/impl/FsDatasetUtil.java     |   14 +++
 .../org/apache/hadoop/hdfs/tools/DebugAdmin.java   |  107 ++++++++++++++++++--
 .../apache/hadoop/hdfs/tools/TestDebugAdmin.java   |   57 ++++++++++-
 5 files changed, 191 insertions(+), 37 deletions(-)

diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSClient.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSClient.java
index ef78c08..018a69d 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSClient.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSClient.java
@@ -563,7 +563,7 @@ public boolean isDomainSocketDataTraffic() {
       return domainSocketDataTraffic;
     }
 
-    private DataChecksum.Type getChecksumType(Configuration conf) {
+    static private DataChecksum.Type getChecksumType(Configuration conf) {
       final String checksum = conf.get(
           DFSConfigKeys.DFS_CHECKSUM_TYPE_KEY,
           DFSConfigKeys.DFS_CHECKSUM_TYPE_DEFAULT);
@@ -586,7 +586,7 @@ public boolean isDomainSocketDataTraffic() {
     }
 
     // Construct a checksum option from conf
-    private ChecksumOpt getChecksumOptFromConf(Configuration conf) {
+    public static ChecksumOpt getChecksumOptFromConf(Configuration conf) {
       DataChecksum.Type type = getChecksumType(conf);
       int bytesPerChecksum = conf.getInt(DFS_BYTES_PER_CHECKSUM_KEY,
           DFS_BYTES_PER_CHECKSUM_DEFAULT);
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java
index c6dad0e..3f265ec 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java
@@ -699,7 +699,7 @@ File getBlockFile(String bpid, long blockId) throws IOException {
     }
     return f;
   }
-  
+
   /**
    * Return the File associated with a block, without first
    * checking that it exists. This should be used when the
@@ -761,7 +761,7 @@ ReplicaInfo getReplicaInfo(ExtendedBlock b)
     }
     return info;
   }
-  
+
   /**
    * Get the meta info of a block stored in volumeMap. Block is looked up
    * without matching the generation stamp.
@@ -780,7 +780,7 @@ private ReplicaInfo getReplicaInfo(String bpid, long blkid)
     }
     return info;
   }
-  
+
   /**
    * Returns handles to the block file and its metadata file
    */
@@ -992,7 +992,7 @@ public ReplicaInfo moveBlockAcrossVolumes(ExtendedBlock block, FsVolumeSpi
    * @param blockFile block file for which the checksum will be computed
    * @throws IOException
    */
-  private static void computeChecksum(File srcMeta, File dstMeta, File blockFile)
+  static void computeChecksum(File srcMeta, File dstMeta, File blockFile)
       throws IOException {
     final DataChecksum checksum = BlockMetadataHeader.readDataChecksum(srcMeta);
     final byte[] data = new byte[1 << 16];
@@ -1055,20 +1055,20 @@ static private void truncateBlock(File blockFile, File metaFile,
           + ") to newlen (=" + newlen + ")");
     }
 
-    DataChecksum dcs = BlockMetadataHeader.readHeader(metaFile).getChecksum(); 
+    DataChecksum dcs = BlockMetadataHeader.readHeader(metaFile).getChecksum();
     int checksumsize = dcs.getChecksumSize();
     int bpc = dcs.getBytesPerChecksum();
     long n = (newlen - 1)/bpc + 1;
     long newmetalen = BlockMetadataHeader.getHeaderSize() + n*checksumsize;
     long lastchunkoffset = (n - 1)*bpc;
-    int lastchunksize = (int)(newlen - lastchunkoffset); 
-    byte[] b = new byte[Math.max(lastchunksize, checksumsize)]; 
+    int lastchunksize = (int)(newlen - lastchunkoffset);
+    byte[] b = new byte[Math.max(lastchunksize, checksumsize)];
 
     RandomAccessFile blockRAF = new RandomAccessFile(blockFile, "rw");
     try {
-      //truncate blockFile 
+      //truncate blockFile
       blockRAF.setLength(newlen);
- 
+
       //read last chunk
       blockRAF.seek(lastchunkoffset);
       blockRAF.readFully(b, 0, lastchunksize);
@@ -1080,7 +1080,7 @@ static private void truncateBlock(File blockFile, File metaFile,
     dcs.update(b, 0, lastchunksize);
     dcs.writeValue(b, 0, false);
 
-    //update metaFile 
+    //update metaFile
     RandomAccessFile metaRAF = new RandomAccessFile(metaFile, "rw");
     try {
       metaRAF.setLength(newmetalen);
@@ -1323,13 +1323,13 @@ public Replica recoverClose(ExtendedBlock b, long newGS,
   /**
    * Bump a replica's generation stamp to a new one.
    * Its on-disk meta file name is renamed to be the new one too.
-   * 
+   *
    * @param replicaInfo a replica
    * @param newGS new generation stamp
    * @throws IOException if rename fails
    */
-  private void bumpReplicaGS(ReplicaInfo replicaInfo, 
-      long newGS) throws IOException { 
+  private void bumpReplicaGS(ReplicaInfo replicaInfo,
+      long newGS) throws IOException {
     long oldGS = replicaInfo.getGenerationStamp();
     File oldmeta = replicaInfo.getMetaFile();
     replicaInfo.setGenerationStamp(newGS);
@@ -1391,7 +1391,7 @@ public synchronized ReplicaHandler createRbw(
       throw e;
     }
 
-    ReplicaBeingWritten newReplicaInfo = new ReplicaBeingWritten(b.getBlockId(), 
+    ReplicaBeingWritten newReplicaInfo = new ReplicaBeingWritten(b.getBlockId(),
         b.getGenerationStamp(), v, f.getParentFile(), b.getNumBytes());
     volumeMap.add(b.getBlockPoolId(), newReplicaInfo);
     return new ReplicaHandler(newReplicaInfo, ref);
@@ -1407,7 +1407,7 @@ public ReplicaHandler recoverRbw(
       try {
         synchronized (this) {
           ReplicaInfo replicaInfo = getReplicaInfo(b.getBlockPoolId(), b.getBlockId());
-          
+
           // check the replica's state
           if (replicaInfo.getState() != ReplicaState.RBW) {
             throw new ReplicaNotFoundException(
@@ -1877,15 +1877,15 @@ File validateBlockFile(String bpid, long blockId) {
     synchronized(this) {
       f = getFile(bpid, blockId, false);
     }
-    
+
     if(f != null ) {
       if(f.exists())
         return f;
-   
+
       // if file is not null, but doesn't exist - possibly disk failed
       datanode.checkDiskErrorAsync();
     }
-    
+
     if (LOG.isDebugEnabled()) {
       LOG.debug("blockId=" + blockId + ", f=" + f);
     }
@@ -2128,7 +2128,7 @@ File getFile(final String bpid, final long blockId, boolean touch) {
       }
       return info.getBlockFile();
     }
-    return null;    
+    return null;
   }
 
   /**
@@ -2277,7 +2277,7 @@ public void checkAndUpdate(String bpid, long blockId, File diskFile,
        */
       if (memBlockInfo == null) {
         // Block is missing in memory - add the block to volumeMap
-        ReplicaInfo diskBlockInfo = new FinalizedReplica(blockId, 
+        ReplicaInfo diskBlockInfo = new FinalizedReplica(blockId,
             diskFile.length(), diskGS, vol, diskFile.getParentFile());
         volumeMap.add(bpid, diskBlockInfo);
         if (vol.isTransientStorage()) {
@@ -2706,7 +2706,7 @@ public HdfsBlocksMetadata getHdfsBlocksMetadata(String poolId,
     for (int i = 0; i < curVolumes.size(); i++) {
       blocksVolumeIds.add(ByteBuffer.allocate(4).putInt(i).array());
     }
-    // Determine the index of the VolumeId of each block's volume, by comparing 
+    // Determine the index of the VolumeId of each block's volume, by comparing
     // the block's volume against the enumerated volumes
     for (int i = 0; i < blockIds.length; i++) {
       long blockId = blockIds[i];
@@ -3077,7 +3077,7 @@ public void setPinning(ExtendedBlock block) throws IOException {
 
     File f = getBlockFile(block);
     Path p = new Path(f.getAbsolutePath());
-    
+
     FsPermission oldPermission = localFS.getFileStatus(
         new Path(f.getAbsolutePath())).getPermission();
     //sticky bit is used for pinning purpose
@@ -3092,7 +3092,7 @@ public boolean getPinning(ExtendedBlock block) throws IOException {
       return  false;
     }
     File f = getBlockFile(block);
-        
+
     FileStatus fss = localFS.getFileStatus(new Path(f.getAbsolutePath()));
     return fss.getPermission().getStickyBit();
   }
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetUtil.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetUtil.java
index f129719..ac38eba 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetUtil.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetUtil.java
@@ -22,7 +22,9 @@
 import java.io.IOException;
 import java.util.Arrays;
 
+import com.google.common.base.Preconditions;
 import org.apache.hadoop.classification.InterfaceAudience;
+import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.hdfs.protocol.Block;
 import org.apache.hadoop.hdfs.server.common.GenerationStamp;
 import org.apache.hadoop.hdfs.server.datanode.DatanodeUtil;
@@ -105,4 +107,16 @@ static long parseGenerationStamp(File blockFile, File metaFile
           + blockFile + ", metaFile=" + metaFile, nfe);
     }
   }
+
+  /**
+   * Compute the checksum for a block file that does not already have
+   * its checksum computed, and save it to dstMeta file.
+   */
+  public static void computeChecksum(File srcMeta, File dstMeta, File blockFile)
+      throws IOException {
+    Preconditions.checkNotNull(srcMeta);
+    Preconditions.checkNotNull(dstMeta);
+    Preconditions.checkNotNull(blockFile);
+    FsDatasetImpl.computeChecksum(srcMeta, dstMeta, blockFile);
+  }
 }
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/DebugAdmin.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/DebugAdmin.java
index 7db179d..e068583 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/DebugAdmin.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/DebugAdmin.java
@@ -17,7 +17,11 @@
  */
 package org.apache.hadoop.hdfs.tools;
 
+import java.io.BufferedOutputStream;
 import java.io.DataInputStream;
+import java.io.DataOutputStream;
+import java.io.File;
+import java.io.FileOutputStream;
 import java.io.FileInputStream;
 import java.io.IOException;
 import java.net.URI;
@@ -29,14 +33,19 @@
 import java.util.concurrent.TimeUnit;
 
 import com.google.common.util.concurrent.Uninterruptibles;
+import org.apache.hadoop.HadoopIllegalArgumentException;
 import org.apache.hadoop.classification.InterfaceAudience;
 import org.apache.hadoop.classification.InterfaceStability;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.conf.Configured;
 import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Options;
 import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.hdfs.DFSClient;
 import org.apache.hadoop.hdfs.DistributedFileSystem;
+import org.apache.hadoop.hdfs.protocol.HdfsConstants;
 import org.apache.hadoop.hdfs.server.datanode.BlockMetadataHeader;
+import org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetUtil;
 import org.apache.hadoop.io.IOUtils;
 import org.apache.hadoop.util.DataChecksum;
 import org.apache.hadoop.util.StringUtils;
@@ -55,9 +64,10 @@
    * All the debug commands we can run.
    */
   private DebugCommand DEBUG_COMMANDS[] = {
-    new VerifyBlockChecksumCommand(),
-    new RecoverLeaseCommand(),
-    new HelpCommand()
+      new VerifyMetaCommand(),
+      new ComputeMetaCommand(),
+      new RecoverLeaseCommand(),
+      new HelpCommand()
   };
 
   /**
@@ -82,10 +92,10 @@
   /**
    * The command for verifying a block metadata file and possibly block file.
    */
-  private class VerifyBlockChecksumCommand extends DebugCommand {
-    VerifyBlockChecksumCommand() {
-      super("verify",
-"verify -meta <metadata-file> [-block <block-file>]",
+  private class VerifyMetaCommand extends DebugCommand {
+    VerifyMetaCommand() {
+      super("verifyMeta",
+"verifyMeta -meta <metadata-file> [-block <block-file>]",
 "  Verify HDFS metadata and block files.  If a block file is specified, we\n" +
 "  will verify that the checksums in the metadata file match the block\n" +
 "  file.");
@@ -194,6 +204,86 @@ int run(List<String> args) throws IOException {
   }
 
   /**
+   * The command for verifying a block metadata file and possibly block file.
+   */
+  private class ComputeMetaCommand extends DebugCommand {
+    ComputeMetaCommand() {
+      super("computeMeta",
+          "computeMeta -block <block-file> -out <output-metadata-file>",
+          "  Compute HDFS metadata from the specified block file, and save it"
+              + " to\n  the specified output metadata file.\n\n"
+              + "**NOTE: Use at your own risk!\n If the block file is corrupt"
+              + " and you overwrite it's meta file, \n it will show up"
+              + " as good in HDFS, but you can't read the data.\n"
+              + " Only use as a last measure, and when you are 100% certain"
+              + " the block file is good.");
+    }
+
+    private DataChecksum createChecksum(Options.ChecksumOpt opt) {
+      DataChecksum dataChecksum = DataChecksum
+          .newDataChecksum(opt.getChecksumType(), opt.getBytesPerChecksum());
+      if (dataChecksum == null) {
+        throw new HadoopIllegalArgumentException(
+            "Invalid checksum type: userOpt=" + opt + ", default=" + opt
+                + ", effective=null");
+      }
+      return dataChecksum;
+    }
+
+    int run(List<String> args) throws IOException {
+      if (args.size() == 0) {
+        System.out.println(usageText);
+        System.out.println(helpText + "\n");
+        return 1;
+      }
+      final String name = StringUtils.popOptionWithArgument("-block", args);
+      if (name == null) {
+        System.err.println("You must specify a block file with -block");
+        return 2;
+      }
+      final File blockFile = new File(name);
+      if (!blockFile.exists() || !blockFile.isFile()) {
+        System.err.println("Block file <" + name + "> does not exist "
+            + "or is not a file");
+        return 3;
+      }
+      final String outFile = StringUtils.popOptionWithArgument("-out", args);
+      if (outFile == null) {
+        System.err.println("You must specify a output file with -out");
+        return 4;
+      }
+      final File srcMeta = new File(outFile);
+      if (srcMeta.exists()) {
+        System.err.println("output file already exists!");
+        return 5;
+      }
+
+      DataOutputStream metaOut = null;
+      try {
+        final Configuration conf = new Configuration();
+        final Options.ChecksumOpt checksumOpt =
+            DFSClient.Conf.getChecksumOptFromConf(conf);
+        final DataChecksum checksum = createChecksum(checksumOpt);
+
+        final int smallBufferSize = HdfsConstants.SMALL_BUFFER_SIZE;
+        metaOut = new DataOutputStream(
+            new BufferedOutputStream(new FileOutputStream(srcMeta),
+                smallBufferSize));
+        BlockMetadataHeader.writeHeader(metaOut, checksum);
+        metaOut.close();
+        FsDatasetUtil.computeChecksum(
+            srcMeta, srcMeta, blockFile);
+        System.out.println(
+            "Checksum calculation succeeded on block file " + name
+                + " saved metadata to meta file " + outFile);
+        return 0;
+      } finally {
+        IOUtils.cleanup(null, metaOut);
+      }
+    }
+  }
+
+  /**
    * The command for recovering a file lease.
    */
   private class RecoverLeaseCommand extends DebugCommand {
@@ -347,6 +437,9 @@ public int run(String[] argv) {
 
   private void printUsage() {
     System.out.println("Usage: hdfs debug <command> [arguments]\n");
+    System.out.println("These commands are for advanced users only.\n");
+    System.out.println("Incorrect usages may result in data loss. " +
+        "Use at your own risk.\n");
     for (DebugCommand command : DEBUG_COMMANDS) {
       if (!command.name.equals("help")) {
         System.out.println(command.usageText);
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/tools/TestDebugAdmin.java b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/tools/TestDebugAdmin.java
index 52b194d..c294569 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/tools/TestDebugAdmin.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/tools/TestDebugAdmin.java
@@ -37,8 +37,14 @@
 
 import static org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetTestUtil.*;
 import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertTrue;
 
 public class TestDebugAdmin {
+
+  static private final String TEST_ROOT_DIR =
+      new File(System.getProperty("test.build.data", "/tmp"),
+          TestDebugAdmin.class.getSimpleName()).getAbsolutePath();
+
   private MiniDFSCluster cluster;
   private DistributedFileSystem fs;
   private DebugAdmin admin;
@@ -46,6 +52,9 @@
 
   @Before
   public void setUp() throws Exception {
+    final File testRoot = new File(TEST_ROOT_DIR);
+    testRoot.delete();
+    testRoot.mkdirs();
     Configuration conf = new Configuration();
     cluster = new MiniDFSCluster.Builder(conf).numDataNodes(1).build();
     cluster.waitActive();
@@ -93,27 +102,65 @@ public void testRecoverLease() throws Exception {
   }
 
   @Test(timeout = 60000)
-  public void testVerifyBlockChecksumCommand() throws Exception {
+  public void testVerifyMetaCommand() throws Exception {
     DFSTestUtil.createFile(fs, new Path("/bar"), 1234, (short) 1, 0xdeadbeef);
     FsDatasetSpi<?> fsd = datanode.getFSDataset();
     ExtendedBlock block = DFSTestUtil.getFirstBlock(fs, new Path("/bar"));
     File blockFile = getBlockFile(fsd,
         block.getBlockPoolId(), block.getLocalBlock());
-    assertEquals("ret: 1, You must specify a meta file with -meta",
-        runCmd(new String[]{"verify", "-block", blockFile.getAbsolutePath()}));
+    assertEquals("ret: 1, You must specify a meta file with -meta", runCmd(
+        new String[] {"verifyMeta", "-block", blockFile.getAbsolutePath()}));
     File metaFile = getMetaFile(fsd,
         block.getBlockPoolId(), block.getLocalBlock());
     assertEquals("ret: 0, Checksum type: " +
           "DataChecksum(type=CRC32C, chunkSize=512)",
-        runCmd(new String[]{"verify",
+        runCmd(new String[]{"verifyMeta",
             "-meta", metaFile.getAbsolutePath()}));
     assertEquals("ret: 0, Checksum type: " +
           "DataChecksum(type=CRC32C, chunkSize=512)" +
           "Checksum verification succeeded on block file " +
           blockFile.getAbsolutePath(),
-        runCmd(new String[]{"verify",
+        runCmd(new String[]{"verifyMeta",
             "-meta", metaFile.getAbsolutePath(),
             "-block", blockFile.getAbsolutePath()})
     );
   }
+
+  @Test(timeout = 60000)
+  public void testComputeMetaCommand() throws Exception {
+    DFSTestUtil.createFile(fs, new Path("/bar"), 1234, (short) 1, 0xdeadbeef);
+    FsDatasetSpi<?> fsd = datanode.getFSDataset();
+    ExtendedBlock block = DFSTestUtil.getFirstBlock(fs, new Path("/bar"));
+    File blockFile = getBlockFile(fsd,
+        block.getBlockPoolId(), block.getLocalBlock());
+
+    assertEquals("ret: 1, computeMeta -block <block-file> -out "
+            + "<output-metadata-file>  Compute HDFS metadata from the specified"
+            + " block file, and save it to  the specified output metadata file."
+            + "**NOTE: Use at your own risk! If the block file is corrupt"
+            + " and you overwrite it's meta file,  it will show up"
+            + " as good in HDFS, but you can't read the data."
+            + " Only use as a last measure, and when you are 100% certain"
+            + " the block file is good.",
+        runCmd(new String[] {"computeMeta"}));
+    assertEquals("ret: 2, You must specify a block file with -block",
+        runCmd(new String[] {"computeMeta", "-whatever"}));
+    assertEquals("ret: 3, Block file <bla> does not exist or is not a file",
+        runCmd(new String[] {"computeMeta", "-block", "bla"}));
+    assertEquals("ret: 4, You must specify a output file with -out", runCmd(
+        new String[] {"computeMeta", "-block", blockFile.getAbsolutePath()}));
+    assertEquals("ret: 5, output file already exists!", runCmd(
+        new String[] {"computeMeta", "-block", blockFile.getAbsolutePath(),
+            "-out", blockFile.getAbsolutePath()}));
+
+    File outFile = new File(TEST_ROOT_DIR, "out.meta");
+    outFile.delete();
+    assertEquals("ret: 0, Checksum calculation succeeded on block file " +
+        blockFile.getAbsolutePath() + " saved metadata to meta file " +
+        outFile.getAbsolutePath(), runCmd(new String[] {"computeMeta", "-block",
+        blockFile.getAbsolutePath(), "-out", outFile.getAbsolutePath()}));
+
+    assertTrue(outFile.exists());
+    assertTrue(outFile.length() > 0);
+  }
 }
-- 
1.7.9.5

