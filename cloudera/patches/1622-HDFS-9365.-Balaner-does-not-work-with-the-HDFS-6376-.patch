From 75cda2fb3f8d8550d92d117db38cfd1d790325f4 Mon Sep 17 00:00:00 2001
From: Tsz-Wo Nicholas Sze <szetszwo@hortonworks.com>
Date: Tue, 24 May 2016 12:49:48 -0700
Subject: [PATCH 1622/2848] HDFS-9365. Balaner does not work with the
 HDFS-6376 HA setup.

(cherry picked from commit d3bdea7f7f94332ffe51fb65eec1f219fbf6657f)

Conflicts:
hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSUtil.java

Change-Id: Ifc10a4485bdf76ece500949a42438923369ceb69
---
 .../main/java/org/apache/hadoop/hdfs/DFSUtil.java  |   21 +++++---
 .../hadoop/hdfs/server/balancer/Balancer.java      |    2 +-
 .../org/apache/hadoop/hdfs/server/mover/Mover.java |    2 +-
 .../java/org/apache/hadoop/hdfs/TestDFSUtil.java   |   51 ++++++++++++--------
 .../hadoop/hdfs/server/balancer/TestBalancer.java  |   12 ++---
 .../balancer/TestBalancerWithHANameNodes.java      |    2 +-
 .../TestBalancerWithMultipleNameNodes.java         |    2 +-
 .../server/balancer/TestBalancerWithNodeGroup.java |    4 +-
 .../apache/hadoop/hdfs/server/mover/TestMover.java |   12 ++---
 .../hadoop/hdfs/server/mover/TestStorageMover.java |    2 +-
 10 files changed, 63 insertions(+), 47 deletions(-)

diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSUtil.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSUtil.java
index d62de27..4356c50 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSUtil.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSUtil.java
@@ -954,9 +954,16 @@ public String toString() {
         "nnId=" + namenodeId + ";addr=" + addr + "]";
     }
   }
-  
+
+  /** @return Internal name services specified in the conf. */
+  static Collection<String> getInternalNameServices(Configuration conf) {
+    final Collection<String> ids = conf.getTrimmedStringCollection(
+        DFSConfigKeys.DFS_INTERNAL_NAMESERVICES_KEY);
+    return !ids.isEmpty()? ids: getNameServiceIds(conf);
+  }
+
   /**
-   * Get a URI for each configured nameservice. If a nameservice is
+   * Get a URI for each internal nameservice. If a nameservice is
    * HA-enabled, then the logical URI of the nameservice is returned. If the
    * nameservice is not HA-enabled, then a URI corresponding to an RPC address
    * of the single NN for that nameservice is returned, preferring the service
@@ -966,8 +973,8 @@ public String toString() {
    * @return a collection of all configured NN URIs, preferring service
    *         addresses
    */
-  public static Collection<URI> getNsServiceRpcUris(Configuration conf) {
-    return getNameServiceUris(conf,
+  public static Collection<URI> getInternalNsRpcUris(Configuration conf) {
+    return getNameServiceUris(conf, getInternalNameServices(conf),
         DFSConfigKeys.DFS_NAMENODE_SERVICE_RPC_ADDRESS_KEY,
         DFSConfigKeys.DFS_NAMENODE_RPC_ADDRESS_KEY);
   }
@@ -983,8 +990,8 @@ public String toString() {
    *        nameservices
    * @return a collection of all configured NN URIs
    */
-  public static Collection<URI> getNameServiceUris(Configuration conf,
-      String... keys) {
+  static Collection<URI> getNameServiceUris(Configuration conf,
+      Collection<String> nameServices, String... keys) {
     Set<URI> ret = new HashSet<URI>();
     
     // We're passed multiple possible configuration keys for any given NN or HA
@@ -994,7 +1001,7 @@ public String toString() {
     // keep track of non-preferred keys here.
     Set<URI> nonPreferredUris = new HashSet<URI>();
     
-    for (String nsId : getNameServiceIds(conf)) {
+    for (String nsId : nameServices) {
       if (HAUtil.isHAEnabled(conf, nsId)) {
         // Add the logical URI of the nameservice.
         try {
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java
index ac51e66..d9a375d 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java
@@ -732,7 +732,7 @@ public int run(String[] args) {
       try {
         checkReplicationPolicyCompatibility(conf);
 
-        final Collection<URI> namenodes = DFSUtil.getNsServiceRpcUris(conf);
+        final Collection<URI> namenodes = DFSUtil.getInternalNsRpcUris(conf);
         return Balancer.run(namenodes, parse(args), conf);
       } catch (IOException e) {
         System.out.println(e + ".  Exiting ...");
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/mover/Mover.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/mover/Mover.java
index 95d9483..e47b0a7 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/mover/Mover.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/mover/Mover.java
@@ -623,7 +623,7 @@ private static Options buildCliOptions() {
       } else if (line.hasOption("p")) {
         paths = line.getOptionValues("p");
       }
-      Collection<URI> namenodes = DFSUtil.getNsServiceRpcUris(conf);
+      Collection<URI> namenodes = DFSUtil.getInternalNsRpcUris(conf);
       if (paths == null || paths.length == 0) {
         for (URI namenode : namenodes) {
           map.put(namenode, null);
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSUtil.java b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSUtil.java
index ee86f12..340b1f3 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSUtil.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSUtil.java
@@ -78,6 +78,8 @@
 import org.junit.Before;
 import org.junit.Test;
 
+import com.google.common.collect.Sets;
+
 public class TestDFSUtil {
   
   /**
@@ -528,7 +530,7 @@ public void testHANameNodesWithFederation() throws URISyntaxException {
     assertEquals(null, DFSUtil.getNamenodeNameServiceId(conf));
     assertEquals(null, DFSUtil.getSecondaryNameServiceId(conf));
     
-    Collection<URI> uris = DFSUtil.getNameServiceUris(conf, DFS_NAMENODE_RPC_ADDRESS_KEY);
+    Collection<URI> uris = getInternalNameServiceUris(conf, DFS_NAMENODE_RPC_ADDRESS_KEY);
     assertEquals(2, uris.size());
     assertTrue(uris.contains(new URI("hdfs://ns1")));
     assertTrue(uris.contains(new URI("hdfs://ns2")));
@@ -611,7 +613,13 @@ public void testSubstituteForWildcardAddress() throws IOException {
     assertEquals("127.0.0.1:12345",
         DFSUtil.substituteForWildcardAddress("127.0.0.1:12345", "foo"));
   }
-  
+
+  private static Collection<URI> getInternalNameServiceUris(Configuration conf,
+      String... keys) {
+    final Collection<String> ids = DFSUtil.getInternalNameServices(conf);
+    return DFSUtil.getNameServiceUris(conf, ids, keys);
+  }
+
   /**
    * Test how name service URIs are handled with a variety of configuration
    * settings
@@ -635,8 +643,7 @@ public void testGetNNUris() throws Exception {
     conf.set(DFS_NAMENODE_SERVICE_RPC_ADDRESS_KEY, "hdfs://" + NN2_ADDR);
     conf.set(CommonConfigurationKeys.FS_DEFAULT_NAME_KEY, "hdfs://" + NN1_ADDR);
 
-    Collection<URI> uris = DFSUtil.getNameServiceUris(conf,
-        DFS_NAMENODE_SERVICE_RPC_ADDRESS_KEY,  DFS_NAMENODE_RPC_ADDRESS_KEY);
+    Collection<URI> uris = DFSUtil.getInternalNsRpcUris(conf);
 
     assertEquals("Incorrect number of URIs returned", 2, uris.size());
     assertTrue("Missing URI for name service ns1",
@@ -660,8 +667,7 @@ public void testGetNNUris() throws Exception {
 
     conf.set(CommonConfigurationKeys.FS_DEFAULT_NAME_KEY, "hdfs://" + NN2_ADDR);
 
-    uris = DFSUtil.getNameServiceUris(conf,
-        DFS_NAMENODE_SERVICE_RPC_ADDRESS_KEY,  DFS_NAMENODE_RPC_ADDRESS_KEY);
+    uris = DFSUtil.getInternalNsRpcUris(conf);
 
     assertEquals("Incorrect number of URIs returned", 3, uris.size());
     assertTrue("Missing URI for name service ns1",
@@ -675,8 +681,7 @@ public void testGetNNUris() throws Exception {
     conf.set(CommonConfigurationKeys.FS_DEFAULT_NAME_KEY,
         "viewfs://vfs-name.example.com");
 
-    uris = DFSUtil.getNameServiceUris(conf,
-        DFS_NAMENODE_SERVICE_RPC_ADDRESS_KEY, DFS_NAMENODE_RPC_ADDRESS_KEY);
+    uris = DFSUtil.getInternalNsRpcUris(conf);
 
     assertEquals("Incorrect number of URIs returned", 3, uris.size());
     assertTrue("Missing URI for name service ns1",
@@ -689,9 +694,8 @@ public void testGetNNUris() throws Exception {
     // Make sure that an HA URI being the default URI doesn't result in multiple
     // entries being returned.
     conf.set(CommonConfigurationKeys.FS_DEFAULT_NAME_KEY, "hdfs://ns1");
-    
-    uris = DFSUtil.getNameServiceUris(conf,
-        DFS_NAMENODE_SERVICE_RPC_ADDRESS_KEY, DFS_NAMENODE_RPC_ADDRESS_KEY);
+
+    uris = DFSUtil.getInternalNsRpcUris(conf);
 
     assertEquals("Incorrect number of URIs returned", 3, uris.size());
     assertTrue("Missing URI for name service ns1",
@@ -705,8 +709,7 @@ public void testGetNNUris() throws Exception {
     conf = new HdfsConfiguration();
     conf.set(CommonConfigurationKeys.FS_DEFAULT_NAME_KEY, "hdfs://" + NN1_ADDR);
 
-    uris = DFSUtil.getNameServiceUris(conf,
-        DFS_NAMENODE_SERVICE_RPC_ADDRESS_KEY, DFS_NAMENODE_RPC_ADDRESS_KEY);
+    uris = DFSUtil.getInternalNsRpcUris(conf);
 
     assertEquals("Incorrect number of URIs returned", 1, uris.size());
     assertTrue("Missing URI for RPC address (defaultFS)",
@@ -716,8 +719,7 @@ public void testGetNNUris() throws Exception {
     // and the default FS is given.
     conf.set(DFS_NAMENODE_RPC_ADDRESS_KEY, NN2_ADDR);
 
-    uris = DFSUtil.getNameServiceUris(conf,
-        DFS_NAMENODE_SERVICE_RPC_ADDRESS_KEY, DFS_NAMENODE_RPC_ADDRESS_KEY);
+    uris = DFSUtil.getInternalNsRpcUris(conf);
 
     assertEquals("Incorrect number of URIs returned", 1, uris.size());
     assertTrue("Missing URI for RPC address",
@@ -729,8 +731,7 @@ public void testGetNNUris() throws Exception {
     // returned.
     conf.set(DFS_NAMENODE_SERVICE_RPC_ADDRESS_KEY, NN1_ADDR);
 
-    uris = DFSUtil.getNameServiceUris(conf,
-        DFS_NAMENODE_SERVICE_RPC_ADDRESS_KEY, DFS_NAMENODE_RPC_ADDRESS_KEY);
+    uris = DFSUtil.getInternalNsRpcUris(conf);
 
     assertEquals("Incorrect number of URIs returned", 1, uris.size());
     assertTrue("Missing URI for service ns1",
@@ -741,9 +742,8 @@ public void testGetNNUris() throws Exception {
     conf = new HdfsConfiguration();
     conf.set(CommonConfigurationKeys.FS_DEFAULT_NAME_KEY, "hdfs://" + NN1_ADDR);
     conf.set(DFS_NAMENODE_SERVICE_RPC_ADDRESS_KEY, NN1_SRVC_ADDR);
-    
-    uris = DFSUtil.getNameServiceUris(conf,
-        DFS_NAMENODE_SERVICE_RPC_ADDRESS_KEY, DFS_NAMENODE_RPC_ADDRESS_KEY);
+
+    uris = DFSUtil.getInternalNsRpcUris(conf);
 
     assertEquals("Incorrect number of URIs returned", 1, uris.size());
     assertTrue("Missing URI for service address",
@@ -759,7 +759,7 @@ public void testLocalhostReverseLookup() {
     // it will automatically convert it to hostname
     HdfsConfiguration conf = new HdfsConfiguration();
     conf.set(CommonConfigurationKeys.FS_DEFAULT_NAME_KEY, "hdfs://127.0.0.1:8020");
-    Collection<URI> uris = DFSUtil.getNameServiceUris(conf);
+    Collection<URI> uris = getInternalNameServiceUris(conf);
     assertEquals(1, uris.size());
     for (URI uri : uris) {
       assertThat(uri.getHost(), not("127.0.0.1"));
@@ -946,10 +946,19 @@ public void testGetNNServiceRpcAddressesForNsIds() throws IOException {
     conf.set(DFSUtil.addKeySuffixes(DFS_NAMENODE_RPC_ADDRESS_KEY, "nn2"),
             NN2_ADDRESS);
 
+    {
+      Collection<String> internal = DFSUtil.getInternalNameServices(conf);
+      assertEquals(Sets.newHashSet("nn1"), internal);
+
+      Collection<String> all = DFSUtil.getNameServiceIds(conf);
+      assertEquals(Sets.newHashSet("nn1", "nn2"), all);
+    }
+
     Map<String, Map<String, InetSocketAddress>> nnMap = DFSUtil
             .getNNServiceRpcAddressesForCluster(conf);
     assertEquals(1, nnMap.size());
     assertTrue(nnMap.containsKey("nn1"));
+
     conf.set(DFS_INTERNAL_NAMESERVICES_KEY, "nn3");
     try {
       DFSUtil.getNNServiceRpcAddressesForCluster(conf);
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/balancer/TestBalancer.java b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/balancer/TestBalancer.java
index 16dd997..b97e7cc 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/balancer/TestBalancer.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/balancer/TestBalancer.java
@@ -423,7 +423,7 @@ public void testBalancerWithPinnedBlocks() throws Exception {
       waitForHeartBeat(totalUsedSpace, totalCapacity, client, cluster);
 
       // start rebalancing
-      Collection<URI> namenodes = DFSUtil.getNsServiceRpcUris(conf);
+      Collection<URI> namenodes = DFSUtil.getInternalNsRpcUris(conf);
       int r = Balancer.run(namenodes, Balancer.Parameters.DEFAULT, conf);
       assertEquals(ExitStatus.NO_MOVE_PROGRESS.getExitCode(), r);
       
@@ -725,7 +725,7 @@ private void runBalancer(Configuration conf,
     waitForHeartBeat(totalUsedSpace, totalCapacity, client, cluster);
 
     // start rebalancing
-    Collection<URI> namenodes = DFSUtil.getNsServiceRpcUris(conf);
+    Collection<URI> namenodes = DFSUtil.getInternalNsRpcUris(conf);
     final int r = runBalancer(namenodes, p, conf);
     if (conf.getInt(DFSConfigKeys.DFS_DATANODE_BALANCE_MAX_NUM_CONCURRENT_MOVES_KEY, 
         DFSConfigKeys.DFS_DATANODE_BALANCE_MAX_NUM_CONCURRENT_MOVES_DEFAULT) ==0) {
@@ -927,7 +927,7 @@ private void testUnknownDatanode(Configuration conf)
           new String[]{RACK0}, null,new long[]{CAPACITY});
       cluster.triggerHeartbeats();
 
-      Collection<URI> namenodes = DFSUtil.getNsServiceRpcUris(conf);
+      Collection<URI> namenodes = DFSUtil.getInternalNsRpcUris(conf);
       Set<String>  datanodes = new HashSet<String>();
       datanodes.add(cluster.getDataNodes().get(0).getDatanodeId().getHostName());
       Balancer.Parameters p = new Balancer.Parameters(
@@ -1363,7 +1363,7 @@ public void testBalancerWithRamDisk() throws Exception {
         null, null, storageCapacities, null, false, false, false, null);
 
       cluster.triggerHeartbeats();
-      Collection<URI> namenodes = DFSUtil.getNsServiceRpcUris(conf);
+      Collection<URI> namenodes = DFSUtil.getInternalNsRpcUris(conf);
 
       // Run Balancer
       final Balancer.Parameters p = Parameters.DEFAULT;
@@ -1413,7 +1413,7 @@ public void testBalancerDuringUpgrade() throws Exception {
       // Add another DN with the same capacity, cluster is now unbalanced
       cluster.startDataNodes(conf, 1, true, null, null);
       cluster.triggerHeartbeats();
-      Collection<URI> namenodes = DFSUtil.getNsServiceRpcUris(conf);
+      Collection<URI> namenodes = DFSUtil.getInternalNsRpcUris(conf);
 
       // Run balancer
       final Balancer.Parameters p = Parameters.DEFAULT;
@@ -1499,7 +1499,7 @@ public void testTwoReplicaShouldNotInSameDN() throws Exception {
       cluster.triggerHeartbeats();
 
       Balancer.Parameters p = Balancer.Parameters.DEFAULT;
-      Collection<URI> namenodes = DFSUtil.getNsServiceRpcUris(conf);
+      Collection<URI> namenodes = DFSUtil.getInternalNsRpcUris(conf);
       final int r = Balancer.run(namenodes, p, conf);
 
       // Replica in (DN0,SSD) was not moved to (DN1,SSD), because (DN1,DISK)
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/balancer/TestBalancerWithHANameNodes.java b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/balancer/TestBalancerWithHANameNodes.java
index bd91366..6704982 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/balancer/TestBalancerWithHANameNodes.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/balancer/TestBalancerWithHANameNodes.java
@@ -94,7 +94,7 @@ public void testBalancerWithHANameNodes() throws Exception {
       totalCapacity += newNodeCapacity;
       TestBalancer.waitForHeartBeat(totalUsedSpace, totalCapacity, client,
           cluster);
-      Collection<URI> namenodes = DFSUtil.getNsServiceRpcUris(conf);
+      Collection<URI> namenodes = DFSUtil.getInternalNsRpcUris(conf);
       assertEquals(1, namenodes.size());
       assertTrue(namenodes.contains(HATestUtil.getLogicalUri(cluster)));
       final int r = Balancer.run(namenodes, Balancer.Parameters.DEFAULT, conf);
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/balancer/TestBalancerWithMultipleNameNodes.java b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/balancer/TestBalancerWithMultipleNameNodes.java
index f51757c..62d5b1f 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/balancer/TestBalancerWithMultipleNameNodes.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/balancer/TestBalancerWithMultipleNameNodes.java
@@ -152,7 +152,7 @@ static void runBalancer(Suite s,
     LOG.info("BALANCER 1");
 
     // start rebalancing
-    final Collection<URI> namenodes = DFSUtil.getNsServiceRpcUris(s.conf);
+    final Collection<URI> namenodes = DFSUtil.getInternalNsRpcUris(s.conf);
     final int r = Balancer.run(namenodes, Balancer.Parameters.DEFAULT, s.conf);
     Assert.assertEquals(ExitStatus.SUCCESS.getExitCode(), r);
 
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/balancer/TestBalancerWithNodeGroup.java b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/balancer/TestBalancerWithNodeGroup.java
index 7af3a0e..d6280a3 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/balancer/TestBalancerWithNodeGroup.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/balancer/TestBalancerWithNodeGroup.java
@@ -174,7 +174,7 @@ private void runBalancer(Configuration conf,
     waitForHeartBeat(totalUsedSpace, totalCapacity);
 
     // start rebalancing
-    Collection<URI> namenodes = DFSUtil.getNsServiceRpcUris(conf);
+    Collection<URI> namenodes = DFSUtil.getInternalNsRpcUris(conf);
     final int r = Balancer.run(namenodes, Balancer.Parameters.DEFAULT, conf);
     assertEquals(ExitStatus.SUCCESS.getExitCode(), r);
 
@@ -188,7 +188,7 @@ private void runBalancerCanFinish(Configuration conf,
     waitForHeartBeat(totalUsedSpace, totalCapacity);
 
     // start rebalancing
-    Collection<URI> namenodes = DFSUtil.getNsServiceRpcUris(conf);
+    Collection<URI> namenodes = DFSUtil.getInternalNsRpcUris(conf);
     final int r = Balancer.run(namenodes, Balancer.Parameters.DEFAULT, conf);
     Assert.assertTrue(r == ExitStatus.SUCCESS.getExitCode() ||
         (r == ExitStatus.NO_MOVE_PROGRESS.getExitCode()));
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/mover/TestMover.java b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/mover/TestMover.java
index d09989b..8418c45 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/mover/TestMover.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/mover/TestMover.java
@@ -47,7 +47,7 @@
 
 public class TestMover {
   static Mover newMover(Configuration conf) throws IOException {
-    final Collection<URI> namenodes = DFSUtil.getNsServiceRpcUris(conf);
+    final Collection<URI> namenodes = DFSUtil.getInternalNsRpcUris(conf);
     Assert.assertEquals(1, namenodes.size());
     Map<URI, List<Path>> nnMap = Maps.newHashMap();
     for (URI nn : namenodes) {
@@ -165,7 +165,7 @@ public void testMoverCli() throws Exception {
       }
 
       Map<URI, List<Path>> movePaths = Mover.Cli.getNameNodePathsToMove(conf);
-      Collection<URI> namenodes = DFSUtil.getNsServiceRpcUris(conf);
+      Collection<URI> namenodes = DFSUtil.getInternalNsRpcUris(conf);
       Assert.assertEquals(1, namenodes.size());
       Assert.assertEquals(1, movePaths.size());
       URI nn = namenodes.iterator().next();
@@ -173,7 +173,7 @@ public void testMoverCli() throws Exception {
       Assert.assertNull(movePaths.get(nn));
 
       movePaths = Mover.Cli.getNameNodePathsToMove(conf, "-p", "/foo", "/bar");
-      namenodes = DFSUtil.getNsServiceRpcUris(conf);
+      namenodes = DFSUtil.getInternalNsRpcUris(conf);
       Assert.assertEquals(1, movePaths.size());
       nn = namenodes.iterator().next();
       Assert.assertTrue(movePaths.containsKey(nn));
@@ -194,7 +194,7 @@ public void testMoverCliWithHAConf() throws Exception {
     try {
       Map<URI, List<Path>> movePaths = Mover.Cli.getNameNodePathsToMove(conf,
           "-p", "/foo", "/bar");
-      Collection<URI> namenodes = DFSUtil.getNsServiceRpcUris(conf);
+      Collection<URI> namenodes = DFSUtil.getInternalNsRpcUris(conf);
       Assert.assertEquals(1, namenodes.size());
       Assert.assertEquals(1, movePaths.size());
       URI nn = namenodes.iterator().next();
@@ -215,7 +215,7 @@ public void testMoverCliWithFederation() throws Exception {
     final Configuration conf = new HdfsConfiguration();
     DFSTestUtil.setFederatedConfiguration(cluster, conf);
     try {
-      Collection<URI> namenodes = DFSUtil.getNsServiceRpcUris(conf);
+      Collection<URI> namenodes = DFSUtil.getInternalNsRpcUris(conf);
       Assert.assertEquals(3, namenodes.size());
 
       try {
@@ -263,7 +263,7 @@ public void testMoverCliWithFederationHA() throws Exception {
     final Configuration conf = new HdfsConfiguration();
     DFSTestUtil.setFederatedHAConfiguration(cluster, conf);
     try {
-      Collection<URI> namenodes = DFSUtil.getNsServiceRpcUris(conf);
+      Collection<URI> namenodes = DFSUtil.getInternalNsRpcUris(conf);
       Assert.assertEquals(3, namenodes.size());
 
       Iterator<URI> iter = namenodes.iterator();
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/mover/TestStorageMover.java b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/mover/TestStorageMover.java
index dda40ab..5596b17 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/mover/TestStorageMover.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/mover/TestStorageMover.java
@@ -271,7 +271,7 @@ void verify(boolean verifyAll) throws Exception {
     }
 
     private void runMover() throws Exception {
-      Collection<URI> namenodes = DFSUtil.getNsServiceRpcUris(conf);
+      Collection<URI> namenodes = DFSUtil.getInternalNsRpcUris(conf);
       Map<URI, List<Path>> nnMap = Maps.newHashMap();
       for (URI nn : namenodes) {
         nnMap.put(nn, null);
-- 
1.7.9.5

