From 52348b85c775885b718f4b26cad6d42ea6249539 Mon Sep 17 00:00:00 2001
From: Sean Mackrory <sean@cloudera.com>
Date: Thu, 24 Aug 2017 16:25:42 -0600
Subject: [PATCH 2622/2848] Revert "HDFS-11445. FSCK shows overall health
 stauts as corrupt even one replica is corrupt.
 Contributed by Brahma Reddy Battula."

This reverts commit 0a8e41ded07b866afc879a7ed90ea83c78e28076.

Change-Id: I20fd4b8a1a2bc805e887e95914e898d256959625
---
 .../BlockInfoUnderConstruction.java                |   22 +++++------
 .../hdfs/server/blockmanagement/BlockManager.java  |   32 ++--------------
 .../hadoop/hdfs/server/namenode/FSNamesystem.java  |    3 +-
 .../hadoop/hdfs/server/namenode/TestFsck.java      |   39 --------------------
 4 files changed, 15 insertions(+), 81 deletions(-)

diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockInfoUnderConstruction.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockInfoUnderConstruction.java
index 8267597..a1f9721 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockInfoUnderConstruction.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockInfoUnderConstruction.java
@@ -82,7 +82,7 @@
      * It is not guaranteed, but expected, that the data-node actually has
      * the replica.
      */
-    public DatanodeStorageInfo getExpectedStorageLocation() {
+    private DatanodeStorageInfo getExpectedStorageLocation() {
       return expectedLocation;
     }
 
@@ -233,40 +233,38 @@ public long getBlockRecoveryId() {
    * Process the recorded replicas. When about to commit or finish the
    * pipeline recovery sort out bad replicas.
    * @param genStamp  The final generation stamp for the block.
-   * @return staleReplica's List.
    */
-  public List<ReplicaUnderConstruction> setGenerationStampAndVerifyReplicas(
-      long genStamp) {
+  public void setGenerationStampAndVerifyReplicas(long genStamp) {
     // Set the generation stamp for the block.
     setGenerationStamp(genStamp);
     if (replicas == null)
-      return null;
+      return;
 
-    List<ReplicaUnderConstruction> staleReplicas = new ArrayList<>();
-    // Remove replicas with wrong gen stamp. The replica list is unchanged.
+    // Remove the replicas with wrong gen stamp.
+    // The replica list is unchanged.
     for (ReplicaUnderConstruction r : replicas) {
       if (genStamp != r.getGenerationStamp()) {
-        staleReplicas.add(r);
+        r.getExpectedStorageLocation().removeBlock(this);
+        NameNode.blockStateChangeLog.info("BLOCK* Removing stale replica "
+            + "from location: {}", r.getExpectedStorageLocation());
       }
     }
-    return staleReplicas;
   }
 
   /**
    * Commit block's length and generation stamp as reported by the client.
    * Set block state to {@link BlockUCState#COMMITTED}.
    * @param block - contains client reported block length and generation 
-   * @return staleReplica's List.
    * @throws IOException if block ids are inconsistent.
    */
-  List<ReplicaUnderConstruction> commitBlock(Block block) throws IOException {
+  void commitBlock(Block block) throws IOException {
     if(getBlockId() != block.getBlockId())
       throw new IOException("Trying to commit inconsistent block: id = "
           + block.getBlockId() + ", expected id = " + getBlockId());
     blockUCState = BlockUCState.COMMITTED;
     this.setNumBytes(block.getNumBytes());
     // Sort out invalid replicas.
-    return setGenerationStampAndVerifyReplicas(block.getGenerationStamp());
+    setGenerationStampAndVerifyReplicas(block.getGenerationStamp());
   }
 
   /**
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java
index 97448b0..f49f500 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java
@@ -70,7 +70,6 @@
 import org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager.AccessMode;
 import org.apache.hadoop.hdfs.security.token.block.DataEncryptionKey;
 import org.apache.hadoop.hdfs.security.token.block.ExportedBlockKeys;
-import org.apache.hadoop.hdfs.server.blockmanagement.BlockInfoUnderConstruction.ReplicaUnderConstruction;
 import org.apache.hadoop.hdfs.server.blockmanagement.CorruptReplicasMap.Reason;
 import org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo.AddBlockResult;
 import org.apache.hadoop.hdfs.server.blockmanagement.PendingDataNodeMessages.ReportedBlockInfo;
@@ -706,7 +705,7 @@ private short getMinMaintenanceStorageNum(BlockInfo block) {
    * @throws IOException if the block does not have at least a minimal number
    * of replicas reported from data-nodes.
    */
-  private boolean commitBlock(final BlockInfoUnderConstruction block,
+  private static boolean commitBlock(final BlockInfoUnderConstruction block,
       final Block commitBlock) throws IOException {
     if (block.getBlockUCState() == BlockUCState.COMMITTED)
       return false;
@@ -717,9 +716,7 @@ private boolean commitBlock(final BlockInfoUnderConstruction block,
       throw new IOException("Commit block with mismatching GS. NN has " +
         block + ", client submits " + commitBlock);
     }
-    List<ReplicaUnderConstruction> staleReplicas =
-        block.commitBlock(commitBlock);
-    removeStaleReplicas(staleReplicas, block);
+    block.commitBlock(commitBlock);
     return true;
   }
   
@@ -815,8 +812,7 @@ private BlockInfo completeBlock(final BlockCollection bc,
    */
   public BlockInfo forceCompleteBlock(final BlockCollection bc,
       final BlockInfoUnderConstruction block) throws IOException {
-    List<ReplicaUnderConstruction> staleReplicas = block.commitBlock(block);
-    removeStaleReplicas(staleReplicas, block);
+    block.commitBlock(block);
     return completeBlock(bc, block, true);
   }
 
@@ -3229,20 +3225,6 @@ public void removeStoredBlock(Block block, DatanodeDescriptor node) {
     }
   }
 
-  private void removeStaleReplicas(List<ReplicaUnderConstruction> staleReplicas,
-      BlockInfoUnderConstruction block) {
-    if (staleReplicas == null) {
-      return;
-    }
-    for (ReplicaUnderConstruction r : staleReplicas) {
-      removeStoredBlock(block,
-          r.getExpectedStorageLocation().getDatanodeDescriptor());
-      NameNode.blockStateChangeLog
-          .info("BLOCK* Removing stale replica " + "from location: {}",
-              r.getExpectedStorageLocation());
-    }
-  }
-
   /**
    * Get all valid locations of the block & add the block to results
    * return the length of the added block; 0 if the block is not added
@@ -3625,14 +3607,6 @@ public BlockInfo getStoredBlock(Block block) {
     return blocksMap.getStoredBlock(block);
   }
 
-  public void updateLastBlock(BlockInfoUnderConstruction lastBlock,
-      ExtendedBlock newBlock) {
-    lastBlock.setNumBytes(newBlock.getNumBytes());
-    List<ReplicaUnderConstruction> staleReplicas = lastBlock
-        .setGenerationStampAndVerifyReplicas(newBlock.getGenerationStamp());
-    removeStaleReplicas(staleReplicas, lastBlock);
-  }
-
   /** updates a block in under replication queue */
   private void updateNeededReplications(final BlockInfo block,
       final int curReplicasDelta, int expectedReplicasDelta) {
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java
index 8884c70..c691790 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java
@@ -7362,7 +7362,8 @@ private void updatePipelineInternal(String clientName, ExtendedBlock oldBlock,
     }
 
     // Update old block with the new generation stamp and new length
-    blockManager.updateLastBlock(blockinfo, newBlock);
+    blockinfo.setNumBytes(newBlock.getNumBytes());
+    blockinfo.setGenerationStampAndVerifyReplicas(newBlock.getGenerationStamp());
 
     // find the DatanodeDescriptor objects
     final DatanodeStorageInfo[] storages = blockManager.getDatanodeManager()
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestFsck.java b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestFsck.java
index 5b0c6ef..c5ae0e5 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestFsck.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestFsck.java
@@ -68,7 +68,6 @@
 import org.apache.hadoop.hdfs.DistributedFileSystem;
 import org.apache.hadoop.hdfs.HdfsConfiguration;
 import org.apache.hadoop.hdfs.MiniDFSCluster;
-import org.apache.hadoop.hdfs.MiniDFSNNTopology;
 import org.apache.hadoop.hdfs.protocol.Block;
 import org.apache.hadoop.hdfs.protocol.CorruptFileBlocks;
 import org.apache.hadoop.hdfs.protocol.DatanodeInfo;
@@ -81,7 +80,6 @@
 import org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor;
 import org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager;
 import org.apache.hadoop.hdfs.server.namenode.NamenodeFsck.Result;
-import org.apache.hadoop.hdfs.server.namenode.ha.HATestUtil;
 import org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols;
 import org.apache.hadoop.test.GenericTestUtils;
 import org.apache.hadoop.hdfs.tools.DFSck;
@@ -89,7 +87,6 @@
 import org.apache.hadoop.net.NetworkTopology;
 import org.apache.hadoop.security.AccessControlException;
 import org.apache.hadoop.security.UserGroupInformation;
-import org.apache.hadoop.test.GenericTestUtils;
 import org.apache.hadoop.util.ToolRunner;
 import org.apache.log4j.Level;
 import org.apache.log4j.Logger;
@@ -1949,40 +1946,4 @@ public Boolean get() {
     assertFalse(fsckOut.contains("InMaintenanceReplicas"));
   }
 
-  @Test(timeout = 300000)
-  public void testFsckCorruptWhenOneReplicaIsCorrupt()
-      throws Exception {
-    Configuration conf = new HdfsConfiguration();
-    final MiniDFSCluster cluster = new MiniDFSCluster.Builder(conf)
-        .nnTopology(MiniDFSNNTopology.simpleHATopology()).numDataNodes(2)
-        .build();
-    try {
-      cluster.waitActive();
-      FileSystem fs = HATestUtil.configureFailoverFs(cluster, conf);
-      cluster.transitionToActive(0);
-      String filePath = "/appendTest";
-      Path fileName = new Path(filePath);
-      DFSTestUtil.createFile(fs, fileName, 512, (short) 2, 0);
-      DFSTestUtil.waitReplication(fs, fileName, (short) 2);
-      assertTrue("File not created", fs.exists(fileName));
-      cluster.getDataNodes().get(1).shutdown();
-      DFSTestUtil.appendFile(fs, fileName, "appendCorruptBlock");
-      cluster.restartDataNode(1, true);
-      GenericTestUtils.waitFor(new Supplier<Boolean>() {
-        @Override
-        public Boolean get() {
-          return (
-              cluster.getNameNode(0).getNamesystem().getCorruptReplicaBlocks()
-                  > 0);
-        }
-      }, 100, 5000);
-
-      DFSTestUtil.appendFile(fs, fileName, "appendCorruptBlock");
-      runFsck(cluster.getConfiguration(0), 0, true, "/");
-    }finally {
-      if(cluster!=null){
-        cluster.shutdown();
-      }
-    }
-  }
 }
-- 
1.7.9.5

