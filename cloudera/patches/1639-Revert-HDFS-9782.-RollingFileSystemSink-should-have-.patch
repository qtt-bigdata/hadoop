From 8f07dd755d341759cae07207704a939b914db653 Mon Sep 17 00:00:00 2001
From: Daniel Templeton <daniel@cloudera.com>
Date: Tue, 28 Jun 2016 14:14:09 -0700
Subject: [PATCH 1639/2848] Revert "HDFS-9782. RollingFileSystemSink should
 have configurable roll interval. (Daniel
 Templeton via kasha)"

This reverts commit e478671e4089a1ff896e4fa587eb4524411600a0.

Change-Id: I46091a15b7a181256294b9ea47cdf4695dd43075
---
 .../metrics2/sink/RollingFileSystemSink.java       |  353 ++++----------------
 .../sink/RollingFileSystemSinkTestBase.java        |   16 +-
 .../metrics2/sink/TestRollingFileSystemSink.java   |  285 +++++-----------
 .../sink/TestRollingFileSystemSinkWithLocal.java   |  157 ---------
 .../sink/TestRollingFileSystemSinkWithHdfs.java    |   26 +-
 .../TestRollingFileSystemSinkWithSecureHdfs.java   |  201 ++++++-----
 6 files changed, 279 insertions(+), 759 deletions(-)
 delete mode 100644 hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/metrics2/sink/TestRollingFileSystemSinkWithLocal.java

diff --git a/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/sink/RollingFileSystemSink.java b/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/sink/RollingFileSystemSink.java
index 9bfd519..9a43901 100644
--- a/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/sink/RollingFileSystemSink.java
+++ b/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/sink/RollingFileSystemSink.java
@@ -31,10 +31,6 @@
 import java.util.TimeZone;
 import java.util.Timer;
 import java.util.TimerTask;
-import java.util.concurrent.ThreadLocalRandom;
-import java.util.concurrent.TimeUnit;
-import java.util.regex.Matcher;
-import java.util.regex.Pattern;
 
 import org.apache.commons.configuration.SubsetConfiguration;
 import org.apache.commons.lang.time.FastDateFormat;
@@ -57,14 +53,14 @@
 /**
  * <p>This class is a metrics sink that uses
  * {@link org.apache.hadoop.fs.FileSystem} to write the metrics logs.  Every
- * roll interval a new directory will be created under the path specified by the
+ * hour a new directory will be created under the path specified by the
  * <code>basepath</code> property. All metrics will be logged to a file in the
- * current interval's directory in a file named &lt;hostname&gt;.log, where
+ * current hour's directory in a file named &lt;hostname&gt;.log, where
  * &lt;hostname&gt; is the name of the host on which the metrics logging
  * process is running. The base path is set by the
  * <code>&lt;prefix&gt;.sink.&lt;instance&gt;.basepath</code> property.  The
- * time zone used to create the current interval's directory name is GMT.  If
- * the <code>basepath</code> property isn't specified, it will default to
+ * time zone used to create the current hour's directory name is GMT.  If the
+ * <code>basepath</code> property isn't specified, it will default to
  * &quot;/tmp&quot;, which is the temp directory on whatever default file
  * system is configured for the cluster.</p>
  *
@@ -73,26 +69,6 @@
  * writing a log file.  The default value is <code>true</code>.  When set to
  * <code>false</code>, file errors are quietly swallowed.</p>
  *
- * <p>The <code>roll-interval</code> property sets the amount of time before
- * rolling the directory. The default value is 1 hour. The roll interval may
- * not be less than 1 minute. The property's value should be given as
- * <i>number unit</i>, where <i>number</i> is an integer value, and
- * <i>unit</i> is a valid unit.  Valid units are <i>minute</i>, <i>hour</i>,
- * and <i>day</i>.  The units are case insensitive and may be abbreviated or
- * plural. If no units are specified, hours are assumed. For example,
- * &quot;2&quot;, &quot;2h&quot;, &quot;2 hour&quot;, and
- * &quot;2 hours&quot; are all valid ways to specify two hours.</p>
- *
- * <p>The <code>roll-offset-interval-millis</code> property sets the upper
- * bound on a random time interval (in milliseconds) that is used to delay
- * before the initial roll.  All subsequent rolls will happen an integer
- * number of roll intervals after the initial roll, hence retaining the original
- * offset. The purpose of this property is to insert some variance in the roll
- * times so that large clusters using this sink on every node don't cause a
- * performance impact on HDFS by rolling simultaneously.  The default value is
- * 30000 (30s).  When writing to HDFS, as a rule of thumb, the roll offset in
- * millis should be no less than the number of sink instances times 5.
- *
  * <p>The primary use of this class is for logging to HDFS.  As it uses
  * {@link org.apache.hadoop.fs.FileSystem} to access the target file system,
  * however, it can be used to write to the local file system, Amazon S3, or any
@@ -103,8 +79,7 @@
  * <p>Not all file systems support the ability to append to files.  In file
  * systems without the ability to append to files, only one writer can write to
  * a file at a time.  To allow for concurrent writes from multiple daemons on a
- * single host, the <code>source</code> property is used to set unique headers
- * for the log files.  The property should be set to the name of
+ * single host, the <code>source</code> property should be set to the name of
  * the source daemon, e.g. <i>namenode</i>.  The value of the
  * <code>source</code> property should typically be the same as the property's
  * prefix.  If this property is not set, the source is taken to be
@@ -130,7 +105,7 @@
  * 3.</p>
  *
  * <p>Note also that when writing to HDFS, the file size information is not
- * updated until the file is closed (at the end of the interval) even though
+ * updated until the file is closed (e.g. at the top of the hour) even though
  * the data is being written successfully. This is a known HDFS limitation that
  * exists because of the performance cost of updating the metadata.  See
  * <a href="https://issues.apache.org/jira/browse/HDFS-5478">HDFS-5478</a>.</p>
@@ -149,32 +124,21 @@
   private static final String BASEPATH_KEY = "basepath";
   private static final String SOURCE_KEY = "source";
   private static final String IGNORE_ERROR_KEY = "ignore-error";
-  private static final boolean DEFAULT_IGNORE_ERROR = false;
   private static final String ALLOW_APPEND_KEY = "allow-append";
-  private static final boolean DEFAULT_ALLOW_APPEND = false;
   private static final String KEYTAB_PROPERTY_KEY = "keytab-key";
   private static final String USERNAME_PROPERTY_KEY = "principal-key";
-  private static final String ROLL_INTERVAL_KEY = "roll-interval";
-  private static final String DEFAULT_ROLL_INTERVAL = "1h";
-  private static final String ROLL_OFFSET_INTERVAL_MILLIS_KEY =
-      "roll-offset-interval-millis";
-  private static final int DEFAULT_ROLL_OFFSET_INTERVAL_MILLIS = 30000;
   private static final String SOURCE_DEFAULT = "unknown";
   private static final String BASEPATH_DEFAULT = "/tmp";
   private static final FastDateFormat DATE_FORMAT =
-      FastDateFormat.getInstance("yyyyMMddHHmm", TimeZone.getTimeZone("GMT"));
+      FastDateFormat.getInstance("yyyyMMddHH", TimeZone.getTimeZone("GMT"));
   private final Object lock = new Object();
   private boolean initialized = false;
   private SubsetConfiguration properties;
   private Configuration conf;
-  @VisibleForTesting
-  protected String source;
-  @VisibleForTesting
-  protected boolean ignoreError;
-  @VisibleForTesting
-  protected boolean allowAppend;
-  @VisibleForTesting
-  protected Path basePath;
+  private String source;
+  private boolean ignoreError;
+  private boolean allowAppend;
+  private Path basePath;
   private FileSystem fileSystem;
   // The current directory path into which we're writing files
   private Path currentDirPath;
@@ -185,21 +149,11 @@
   // We keep this only to be able to call hsynch() on it.
   private FSDataOutputStream currentFSOutStream;
   private Timer flushTimer;
-  // The amount of time between rolls
-  @VisibleForTesting
-  protected long rollIntervalMillis;
-  // The maximum amount of random time to add to the initial roll
-  @VisibleForTesting
-  protected long rollOffsetIntervalMillis;
-  // The time for the nextFlush
-  @VisibleForTesting
-  protected Calendar nextFlush = null;
-  // This flag when true causes a metrics write to schedule a flush thread to
-  // run immediately, but only if a flush thread is already scheduled. (It's a
-  // timing thing.  If the first write forces the flush, it will strand the
-  // second write.)
+
+  // This flag is used during testing to make the flusher thread run after only
+  // a short pause instead of waiting for the top of the hour.
   @VisibleForTesting
-  protected static boolean forceFlush = false;
+  protected static boolean flushQuickly = false;
   // This flag is used by the flusher thread to indicate that it has run. Used
   // only for testing purposes.
   @VisibleForTesting
@@ -211,36 +165,13 @@
   @VisibleForTesting
   protected static FileSystem suppliedFilesystem = null;
 
-  /**
-   * Create an empty instance.  Required for reflection.
-   */
-  public RollingFileSystemSink() {
-  }
-
-  /**
-   * Create an instance for testing.
-   *
-   * @param flushIntervalMillis the roll interval in millis
-   * @param flushOffsetIntervalMillis the roll offset interval in millis
-   */
-  @VisibleForTesting
-  protected RollingFileSystemSink(long flushIntervalMillis,
-      long flushOffsetIntervalMillis) {
-    this.rollIntervalMillis = flushIntervalMillis;
-    this.rollOffsetIntervalMillis = flushOffsetIntervalMillis;
-  }
-
   @Override
   public void init(SubsetConfiguration metrics2Properties) {
     properties = metrics2Properties;
     basePath = new Path(properties.getString(BASEPATH_KEY, BASEPATH_DEFAULT));
     source = properties.getString(SOURCE_KEY, SOURCE_DEFAULT);
-    ignoreError = properties.getBoolean(IGNORE_ERROR_KEY, DEFAULT_IGNORE_ERROR);
-    allowAppend = properties.getBoolean(ALLOW_APPEND_KEY, DEFAULT_ALLOW_APPEND);
-    rollOffsetIntervalMillis =
-        getNonNegative(ROLL_OFFSET_INTERVAL_MILLIS_KEY,
-          DEFAULT_ROLL_OFFSET_INTERVAL_MILLIS);
-    rollIntervalMillis = getRollInterval();
+    ignoreError = properties.getBoolean(IGNORE_ERROR_KEY, false);
+    allowAppend = properties.getBoolean(ALLOW_APPEND_KEY, false);
 
     conf = loadConf();
     UserGroupInformation.setConfiguration(conf);
@@ -248,8 +179,8 @@ public void init(SubsetConfiguration metrics2Properties) {
     // Don't do secure setup if it's not needed.
     if (UserGroupInformation.isSecurityEnabled()) {
       // Validate config so that we don't get an NPE
-      checkIfPropertyExists(KEYTAB_PROPERTY_KEY);
-      checkIfPropertyExists(USERNAME_PROPERTY_KEY);
+      checkForProperty(properties, KEYTAB_PROPERTY_KEY);
+      checkForProperty(properties, USERNAME_PROPERTY_KEY);
 
 
       try {
@@ -297,7 +228,6 @@ private boolean initFs() {
       }
 
       flushTimer = new Timer("RollingFileSystemSink Flusher", true);
-      setInitialFlushTime(new Date());
     }
 
     return success;
@@ -308,6 +238,8 @@ private boolean initFs() {
    * strings, allowing for either the property or the configuration not to be
    * set.
    *
+   * @param properties the sink properties
+   * @param conf the conf
    * @param property the property to stringify
    * @return the stringified property
    */
@@ -333,97 +265,14 @@ private String stringifySecurityProperty(String property) {
   }
 
   /**
-   * Extract the roll interval from the configuration and return it in
-   * milliseconds.
-   *
-   * @return the roll interval in millis
-   */
-  @VisibleForTesting
-  protected long getRollInterval() {
-    String rollInterval =
-        properties.getString(ROLL_INTERVAL_KEY, DEFAULT_ROLL_INTERVAL);
-    Pattern pattern = Pattern.compile("^\\s*(\\d+)\\s*([A-Za-z]*)\\s*$");
-    Matcher match = pattern.matcher(rollInterval);
-    long millis;
-
-    if (match.matches()) {
-      String flushUnit = match.group(2);
-      int rollIntervalInt;
-
-      try {
-        rollIntervalInt = Integer.parseInt(match.group(1));
-      } catch (NumberFormatException ex) {
-        throw new MetricsException("Unrecognized flush interval: "
-            + rollInterval + ". Must be a number followed by an optional "
-            + "unit. The unit must be one of: minute, hour, day", ex);
-      }
-
-      if ("".equals(flushUnit)) {
-        millis = TimeUnit.HOURS.toMillis(rollIntervalInt);
-      } else {
-        switch (flushUnit.toLowerCase()) {
-        case "m":
-        case "min":
-        case "minute":
-        case "minutes":
-          millis = TimeUnit.MINUTES.toMillis(rollIntervalInt);
-          break;
-        case "h":
-        case "hr":
-        case "hour":
-        case "hours":
-          millis = TimeUnit.HOURS.toMillis(rollIntervalInt);
-          break;
-        case "d":
-        case "day":
-        case "days":
-          millis = TimeUnit.DAYS.toMillis(rollIntervalInt);
-          break;
-        default:
-          throw new MetricsException("Unrecognized unit for flush interval: "
-              + flushUnit + ". Must be one of: minute, hour, day");
-        }
-      }
-    } else {
-      throw new MetricsException("Unrecognized flush interval: "
-          + rollInterval + ". Must be a number followed by an optional unit."
-          + " The unit must be one of: minute, hour, day");
-    }
-
-    if (millis < 60000) {
-      throw new MetricsException("The flush interval property must be "
-          + "at least 1 minute. Value was " + rollInterval);
-    }
-
-    return millis;
-  }
-
-  /**
-   * Return the property value if it's non-negative and throw an exception if
-   * it's not.
-   *
-   * @param key the property key
-   * @param defaultValue the default value
-   */
-  private long getNonNegative(String key, int defaultValue) {
-    int flushOffsetIntervalMillis = properties.getInt(key, defaultValue);
-
-    if (flushOffsetIntervalMillis < 0) {
-      throw new MetricsException("The " + key + " property must be "
-          + "non-negative. Value was " + flushOffsetIntervalMillis);
-    }
-
-    return flushOffsetIntervalMillis;
-  }
-
-  /**
    * Throw a {@link MetricsException} if the given property is not set.
    *
+   * @param conf the configuration to test
    * @param key the key to validate
    */
-  private void checkIfPropertyExists(String key) {
-    if (!properties.containsKey(key)) {
-      throw new MetricsException("Metrics2 configuration is missing " + key
+  private static void checkForProperty(SubsetConfiguration conf, String key) {
+    if (!conf.containsKey(key)) {
+      throw new MetricsException("Configuration is missing " + key
           + " property");
     }
   }
@@ -452,6 +301,7 @@ private Configuration loadConf() {
    * Return the supplied file system for testing or otherwise get a new file
    * system.
    *
+   * @param conf the configuration
    * @return the file system to use
    * @throws MetricsException thrown if the file system could not be retrieved
    */
@@ -477,7 +327,6 @@ private FileSystem getFileSystem() throws MetricsException {
 
   /**
    * Test whether the file system supports append and return the answer.
-   *
    * @param fs the target file system
    */
   private boolean checkAppend(FileSystem fs) {
@@ -502,14 +351,14 @@ private boolean checkAppend(FileSystem fs) {
    * new directory or new log file
    */
   private void rollLogDirIfNeeded() throws MetricsException {
-    // Because we're working relative to the clock, we use a Date instead
-    // of Time.monotonicNow().
     Date now = new Date();
+    String currentDir = DATE_FORMAT.format(now);
+    Path path = new Path(basePath, currentDir);
 
     // We check whether currentOutStream is null instead of currentDirPath,
     // because if currentDirPath is null, then currentOutStream is null, but
-    // currentOutStream can be null for other reasons.  Same for nextFlush.
-    if ((currentOutStream == null) || now.after(nextFlush.getTime())) {
+    // currentOutStream can be null for other reasons.
+    if ((currentOutStream == null) || !path.equals(currentDirPath)) {
       // If we're not yet connected to HDFS, create the connection
       if (!initialized) {
         initialized = initFs();
@@ -523,7 +372,7 @@ private void rollLogDirIfNeeded() throws MetricsException {
           currentOutStream.close();
         }
 
-        currentDirPath = findCurrentDirectory(now);
+        currentDirPath = path;
 
         try {
           rollLogDir();
@@ -531,41 +380,34 @@ private void rollLogDirIfNeeded() throws MetricsException {
           throwMetricsException("Failed to create new log file", ex);
         }
 
-        // Update the time of the next flush
-        updateFlushTime(now);
-        // Schedule the next flush at that time
-        scheduleFlush(nextFlush.getTime());
+        scheduleFlush(now);
       }
-    } else if (forceFlush) {
-      scheduleFlush(new Date());
     }
   }
 
   /**
-   * Use the given time to determine the current directory. The current
-   * directory will be based on the {@link #rollIntervalMinutes}.
+   * Schedule the current hour's directory to be flushed at the top of the next
+   * hour. If this ends up running after the top of the next hour, it will
+   * execute immediately.
    *
    * @param now the current time
-   * @return the current directory
    */
-  private Path findCurrentDirectory(Date now) {
-    long offset = ((now.getTime() - nextFlush.getTimeInMillis())
-        / rollIntervalMillis) * rollIntervalMillis;
-    String currentDir =
-        DATE_FORMAT.format(new Date(nextFlush.getTimeInMillis() + offset));
-
-    return new Path(basePath, currentDir);
-  }
-
-  /**
-   * Schedule the current interval's directory to be flushed. If this ends up
-   * running after the top of the next interval, it will execute immediately.
-   *
-   * @param when the time the thread should run
-   */
-  private void scheduleFlush(Date when) {
+  private void scheduleFlush(Date now) {
     // Store the current currentDirPath to close later
     final PrintStream toClose = currentOutStream;
+    Calendar next = Calendar.getInstance();
+
+    next.setTime(now);
+
+    if (flushQuickly) {
+      // If we're running unit tests, flush after a short pause
+      next.add(Calendar.MILLISECOND, 400);
+    } else {
+      // Otherwise flush at the top of the hour
+      next.set(Calendar.SECOND, 0);
+      next.set(Calendar.MINUTE, 0);
+      next.add(Calendar.HOUR, 1);
+    }
 
     flushTimer.schedule(new TimerTask() {
       @Override
@@ -578,81 +420,11 @@ public void run() {
 
         hasFlushed = true;
       }
-    }, when);
-  }
-
-  /**
-   * Update the {@link #nextFlush} variable to the next flush time. Add
-   * an integer number of flush intervals, preserving the initial random offset.
-   *
-   * @param now the current time
-   */
-  @VisibleForTesting
-  protected void updateFlushTime(Date now) {
-    // In non-initial rounds, add an integer number of intervals to the last
-    // flush until a time in the future is achieved, thus preserving the
-    // original random offset.
-    int millis =
-        (int) (((now.getTime() - nextFlush.getTimeInMillis())
-        / rollIntervalMillis + 1) * rollIntervalMillis);
-
-    nextFlush.add(Calendar.MILLISECOND, millis);
-  }
-
-  /**
-   * Set the {@link #nextFlush} variable to the initial flush time. The initial
-   * flush will be an integer number of flush intervals past the beginning of
-   * the current hour and will have a random offset added, up to
-   * {@link #rollOffsetIntervalMillis}. The initial flush will be a time in
-   * past that can be used from which to calculate future flush times.
-   *
-   * @param now the current time
-   */
-  @VisibleForTesting
-  protected void setInitialFlushTime(Date now) {
-    // Start with the beginning of the current hour
-    nextFlush = Calendar.getInstance();
-    nextFlush.setTime(now);
-    nextFlush.set(Calendar.MILLISECOND, 0);
-    nextFlush.set(Calendar.SECOND, 0);
-    nextFlush.set(Calendar.MINUTE, 0);
-
-    // In the first round, calculate the first flush as the largest number of
-    // intervals from the beginning of the current hour that's not in the
-    // future by:
-    // 1. Subtract the beginning of the hour from the current time
-    // 2. Divide by the roll interval and round down to get the number of whole
-    //    intervals that have passed since the beginning of the hour
-    // 3. Multiply by the roll interval to get the number of millis between
-    //    the beginning of the current hour and the beginning of the current
-    //    interval.
-    int millis = (int) (((now.getTime() - nextFlush.getTimeInMillis())
-        / rollIntervalMillis) * rollIntervalMillis);
-
-    // Then add some noise to help prevent all the nodes from
-    // closing their files at the same time.
-    if (rollOffsetIntervalMillis > 0) {
-      millis += ThreadLocalRandom.current().nextLong(rollOffsetIntervalMillis);
-
-      // If the added time puts us into the future, step back one roll interval
-      // because the code to increment nextFlush to the next flush expects that
-      // nextFlush is the next flush from the previous interval.  There wasn't
-      // a previous interval, so we just fake it with the time in the past that
-      // would have been the previous interval if there had been one.
-      //
-      // It's OK if millis comes out negative.
-      while (nextFlush.getTimeInMillis() + millis > now.getTime()) {
-        millis -= rollIntervalMillis;
-      }
-    }
-
-    // Adjust the next flush time by millis to get the time of our ficticious
-    // previous next flush
-    nextFlush.add(Calendar.MILLISECOND, millis);
+    }, next.getTime());
   }
 
   /**
-   * Create a new directory based on the current interval and a new log file in
+   * Create a new directory based on the current hour and a new log file in
    * that directory.
    *
    * @throws IOException thrown if an error occurs while creating the
@@ -679,8 +451,7 @@ private void rollLogDir() throws IOException {
    * path is found.
    *
    * Once the file is open, update {@link #currentFSOutStream},
-   * {@link #currentOutStream}, and {@#link #currentFilePath} are set
-   * appropriately.
+   * {@link #currentOutStream}, and {@#link #currentFile} are set appropriately.
    *
    * @param initial the target path
    * @throws IOException thrown if the call to see if the exists fails
@@ -781,7 +552,7 @@ private int extractId(String file) {
    * instead.
    *
    * Once the file is open, update {@link #currentFSOutStream},
-   * {@link #currentOutStream}, and {@#link #currentFilePath}.
+   * {@link #currentOutStream}, and {@#link #currentFile} are set appropriately.
    *
    * @param initial the target path
    * @throws IOException thrown if the call to see the append operation fails.
@@ -844,9 +615,9 @@ public void putMetrics(MetricsRecord record) {
         currentOutStream.println();
 
         // If we don't hflush(), the data may not be written until the file is
-        // closed. The file won't be closed until the end of the interval *AND*
+        // closed. The file won't be closed until the top of the hour *AND*
         // another record is received. Calling hflush() makes sure that the data
-        // is complete at the end of the interval.
+        // is complete at the top of the hour.
         try {
           currentFSOutStream.hflush();
         } catch (IOException ex) {
@@ -897,8 +668,8 @@ public void close() {
    * as the new exception's message with the current file name
    * ({@link #currentFilePath}) appended to it.
    *
-   * @param message the exception message. The message will have a colon and
-   * the current file name ({@link #currentFilePath}) appended to it.
+   * @param message the exception message. The message will have the current
+   * file name ({@link #currentFilePath}) appended to it.
    * @throws MetricsException thrown if there was an error and the sink isn't
    * ignoring errors
    */
@@ -916,9 +687,9 @@ private void checkForErrors(String message)
    * ({@link #currentFilePath}) and the Throwable's string representation
    * appended to it.
    *
-   * @param message the exception message. The message will have a colon, the
-   * current file name ({@link #currentFilePath}), and the Throwable's string
-   * representation (wrapped in square brackets) appended to it.
+   * @param message the exception message. The message will have the current
+   * file name ({@link #currentFilePath}) and the Throwable's string
+   * representation appended to it.
    * @param t the Throwable to wrap
    */
   private void throwMetricsException(String message, Throwable t) {
@@ -934,8 +705,8 @@ private void throwMetricsException(String message, Throwable t) {
    * new exception's message with the current file name
    * ({@link #currentFilePath}) appended to it.
    *
-   * @param message the exception message. The message will have a colon and
-   * the current file name ({@link #currentFilePath}) appended to it.
+   * @param message the exception message. The message will have the current
+   * file name ({@link #currentFilePath}) appended to it.
    */
   private void throwMetricsException(String message) {
     if (!ignoreError) {
diff --git a/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/metrics2/sink/RollingFileSystemSinkTestBase.java b/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/metrics2/sink/RollingFileSystemSinkTestBase.java
index 8a315a1..9914c5e 100644
--- a/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/metrics2/sink/RollingFileSystemSinkTestBase.java
+++ b/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/metrics2/sink/RollingFileSystemSinkTestBase.java
@@ -180,12 +180,10 @@ protected MetricsSystem initMetricsSystem(String path, boolean ignoreErrors,
         .add(prefix + ".sink.mysink0.source", "testsrc")
         .add(prefix + ".sink.mysink0.context", "test1")
         .add(prefix + ".sink.mysink0.ignore-error", ignoreErrors)
-        .add(prefix + ".sink.mysink0.allow-append", allowAppend)
-        .add(prefix + ".sink.mysink0.roll-offset-interval-millis", 0)
-        .add(prefix + ".sink.mysink0.roll-interval", "1h");
+        .add(prefix + ".sink.mysink0.allow-append", allowAppend);
 
     if (useSecureParams) {
-      builder.add(prefix + ".sink.mysink0.keytab-key", SINK_KEYTAB_FILE_KEY)
+        builder.add(prefix + ".sink.mysink0.keytab-key", SINK_KEYTAB_FILE_KEY)
         .add(prefix + ".sink.mysink0.principal-key", SINK_PRINCIPAL_KEY);
     }
 
@@ -212,7 +210,7 @@ protected MetricsSystem initMetricsSystem(String path, boolean ignoreErrors,
    */
   protected String doWriteTest(MetricsSystem ms, String path, int count)
       throws IOException, URISyntaxException {
-    final String then = DATE_FORMAT.format(new Date()) + "00";
+    final String then = DATE_FORMAT.format(new Date());
 
     MyMetrics1 mm1 = new MyMetrics1().registerWith(ms);
     new MyMetrics2().registerWith(ms);
@@ -241,7 +239,7 @@ protected String doWriteTest(MetricsSystem ms, String path, int count)
    */
   protected String readLogFile(String path, String then, int count)
       throws IOException, URISyntaxException {
-    final String now = DATE_FORMAT.format(new Date()) + "00";
+    final String now = DATE_FORMAT.format(new Date());
     final String logFile = getLogFilename();
     FileSystem fs = FileSystem.get(new URI(path), new Configuration());
     StringBuilder metrics = new StringBuilder();
@@ -428,7 +426,7 @@ protected void preCreateLogFile(String path, int numFiles)
     Calendar now = getNowNotTopOfHour();
 
     FileSystem fs = FileSystem.get(new URI(path), new Configuration());
-    Path dir = new Path(path, DATE_FORMAT.format(now.getTime()) + "00");
+    Path dir = new Path(path, DATE_FORMAT.format(now.getTime()));
 
     fs.mkdirs(dir);
 
@@ -496,8 +494,8 @@ public void assertFileCount(FileSystem fs, Path dir, int expected)
     }
 
     assertTrue("The sink created additional unexpected log files. " + count
-        + " files were created", expected >= count);
-    assertTrue("The sink created too few log files. " + count + " files were "
+        + "files were created", expected >= count);
+    assertTrue("The sink created too few log files. " + count + "files were "
         + "created", expected <= count);
   }
 
diff --git a/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/metrics2/sink/TestRollingFileSystemSink.java b/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/metrics2/sink/TestRollingFileSystemSink.java
index 9c34dba..3c6cd27 100644
--- a/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/metrics2/sink/TestRollingFileSystemSink.java
+++ b/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/metrics2/sink/TestRollingFileSystemSink.java
@@ -18,244 +18,141 @@
 
 package org.apache.hadoop.metrics2.sink;
 
-import java.util.Calendar;
-import org.apache.commons.configuration.SubsetConfiguration;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.metrics2.MetricsException;
-import org.apache.hadoop.metrics2.impl.ConfigBuilder;
+import org.apache.hadoop.metrics2.MetricsSystem;
 
 import org.junit.Test;
-import static org.junit.Assert.assertEquals;
-import static org.junit.Assert.assertNull;
+import static org.junit.Assert.assertFalse;
+import static org.junit.Assert.assertTrue;
+import static org.junit.Assert.assertFalse;
 import static org.junit.Assert.assertTrue;
-import static org.junit.Assert.fail;
 
 /**
- * Test that the init() method picks up all the configuration settings
- * correctly.
+ * Test the {@link RollingFileSystemSink} class in the context of the local file
+ * system.
  */
-public class TestRollingFileSystemSink {
+public class TestRollingFileSystemSink extends RollingFileSystemSinkTestBase {
+  /**
+   * Test writing logs to the local file system.
+   * @throws Exception when things break
+   */
   @Test
-  public void testInit() {
-    ConfigBuilder builder = new ConfigBuilder();
-    SubsetConfiguration conf =
-        builder.add("sink.roll-interval", "10m")
-            .add("sink.roll-offset-interval-millis", "1")
-            .add("sink.basepath", "path")
-            .add("sink.ignore-error", "true")
-            .add("sink.allow-append", "true")
-            .add("sink.source", "src")
-            .subset("sink");
+  public void testWrite() throws Exception {
+    String path = methodDir.getAbsolutePath();
+    MetricsSystem ms = initMetricsSystem(path, false, false);
 
-    RollingFileSystemSink sink = new RollingFileSystemSink();
-
-    sink.init(conf);
-
-    assertEquals("The roll interval was not set correctly",
-        sink.rollIntervalMillis, 600000);
-    assertEquals("The roll offset interval was not set correctly",
-        sink.rollOffsetIntervalMillis, 1);
-    assertEquals("The base path was not set correctly",
-        sink.basePath, new Path("path"));
-    assertEquals("ignore-error was not set correctly",
-        sink.ignoreError, true);
-    assertEquals("allow-append was not set correctly",
-        sink.allowAppend, true);
-    assertEquals("The source was not set correctly",
-        sink.source, "src");
+    assertMetricsContents(doWriteTest(ms, path, 1));
   }
 
   /**
-   * Test whether the initial roll interval is set correctly.
+   * Test writing logs to the local file system with the sink set to ignore
+   * errors.
+   * @throws Exception when things break
    */
   @Test
-  public void testSetInitialFlushTime() {
-    RollingFileSystemSink rfsSink = new RollingFileSystemSink(1000, 0);
-    Calendar calendar = Calendar.getInstance();
-
-    calendar.set(Calendar.MILLISECOND, 0);
-    calendar.set(Calendar.SECOND, 0);
-    calendar.set(Calendar.MINUTE, 0);
-    calendar.set(Calendar.HOUR, 0);
-    calendar.set(Calendar.DAY_OF_YEAR, 1);
-    calendar.set(Calendar.YEAR, 2016);
-
-    assertNull("Last flush time should have been null prior to calling init()",
-        rfsSink.nextFlush);
-
-    rfsSink.setInitialFlushTime(calendar.getTime());
-
-    long diff =
-        rfsSink.nextFlush.getTimeInMillis() - calendar.getTimeInMillis();
-
-    assertEquals("The initial flush time was calculated incorrectly", 0L, diff);
-
-    calendar.set(Calendar.MILLISECOND, 10);
-    rfsSink.setInitialFlushTime(calendar.getTime());
-    diff = rfsSink.nextFlush.getTimeInMillis() - calendar.getTimeInMillis();
-
-    assertEquals("The initial flush time was calculated incorrectly",
-        -10L, diff);
-
-    calendar.set(Calendar.SECOND, 1);
-    calendar.set(Calendar.MILLISECOND, 10);
-    rfsSink.setInitialFlushTime(calendar.getTime());
-    diff = rfsSink.nextFlush.getTimeInMillis() - calendar.getTimeInMillis();
-
-    assertEquals("The initial flush time was calculated incorrectly",
-        -10L, diff);
-
-    // Try again with a random offset
-    rfsSink = new RollingFileSystemSink(1000, 100);
-
-    assertNull("Last flush time should have been null prior to calling init()",
-        rfsSink.nextFlush);
-
-    calendar.set(Calendar.MILLISECOND, 0);
-    calendar.set(Calendar.SECOND, 0);
-    rfsSink.setInitialFlushTime(calendar.getTime());
-
-    diff = rfsSink.nextFlush.getTimeInMillis() - calendar.getTimeInMillis();
-
-    assertTrue("The initial flush time was calculated incorrectly: " + diff,
-        (diff >= -1000L) && (diff < -900L));
-
-    calendar.set(Calendar.MILLISECOND, 10);
-    rfsSink.setInitialFlushTime(calendar.getTime());
-    diff = rfsSink.nextFlush.getTimeInMillis() - calendar.getTimeInMillis();
-
-    assertTrue("The initial flush time was calculated incorrectly: " + diff,
-        ((diff >= -10L) && (diff <= 0L) ||
-            ((diff > -1000L) && (diff < -910L))));
+  public void testSilentWrite() throws Exception {
+    String path = methodDir.getAbsolutePath();
+    MetricsSystem ms = initMetricsSystem(path, true, false);
 
-    calendar.set(Calendar.SECOND, 1);
-    calendar.set(Calendar.MILLISECOND, 10);
-    rfsSink.setInitialFlushTime(calendar.getTime());
-    diff = rfsSink.nextFlush.getTimeInMillis() - calendar.getTimeInMillis();
-
-    assertTrue("The initial flush time was calculated incorrectly: " + diff,
-        ((diff >= -10L) && (diff <= 0L) ||
-            ((diff > -1000L) && (diff < -910L))));
-
-    // Now try pathological settings
-    rfsSink = new RollingFileSystemSink(1000, 1000000);
-
-    assertNull("Last flush time should have been null prior to calling init()",
-        rfsSink.nextFlush);
-
-    calendar.set(Calendar.MILLISECOND, 1);
-    calendar.set(Calendar.SECOND, 0);
-    rfsSink.setInitialFlushTime(calendar.getTime());
-
-    diff = rfsSink.nextFlush.getTimeInMillis() - calendar.getTimeInMillis();
-
-    assertTrue("The initial flush time was calculated incorrectly: " + diff,
-        (diff > -1000L) && (diff <= 0L));
+    assertMetricsContents(doWriteTest(ms, path, 1));
   }
 
   /**
-   * Test that the roll time updates correctly.
+   * Test writing logs to HDFS when the log file already exists.
+   *
+   * @throws Exception when things break
    */
   @Test
-  public void testUpdateRollTime() {
-    RollingFileSystemSink rfsSink = new RollingFileSystemSink(1000, 0);
-    Calendar calendar = Calendar.getInstance();
-
-    calendar.set(Calendar.MILLISECOND, 0);
-    calendar.set(Calendar.SECOND, 0);
-    calendar.set(Calendar.MINUTE, 0);
-    calendar.set(Calendar.HOUR, 0);
-    calendar.set(Calendar.DAY_OF_YEAR, 1);
-    calendar.set(Calendar.YEAR, 2016);
+  public void testExistingWrite() throws Exception {
+    String path = methodDir.getAbsolutePath();
 
-    rfsSink.nextFlush = Calendar.getInstance();
-    rfsSink.nextFlush.setTime(calendar.getTime());
-    rfsSink.updateFlushTime(calendar.getTime());
+    assertMetricsContents(doAppendTest(path, false, false, 2));
+  }
 
-    assertEquals("The next roll time should have been 1 second in the future",
-        calendar.getTimeInMillis() + 1000,
-        rfsSink.nextFlush.getTimeInMillis());
+  /**
+   * Test writing logs to HDFS when the log file and the .1 log file already
+   * exist.
+   *
+   * @throws Exception when things break
+   */
+  @Test
+  public void testExistingWrite2() throws Exception {
+    String path = methodDir.getAbsolutePath();
+    MetricsSystem ms = initMetricsSystem(path, false, false);
 
-    rfsSink.nextFlush.setTime(calendar.getTime());
-    calendar.add(Calendar.MILLISECOND, 10);
-    rfsSink.updateFlushTime(calendar.getTime());
+    preCreateLogFile(path, 2);
 
-    assertEquals("The next roll time should have been 990 ms in the future",
-        calendar.getTimeInMillis() + 990,
-        rfsSink.nextFlush.getTimeInMillis());
+    assertMetricsContents(doWriteTest(ms, path, 3));
+  }
 
-    rfsSink.nextFlush.setTime(calendar.getTime());
-    calendar.add(Calendar.SECOND, 2);
-    calendar.add(Calendar.MILLISECOND, 10);
-    rfsSink.updateFlushTime(calendar.getTime());
+  /**
+   * Test writing logs to HDFS with ignore errors enabled when
+   * the log file already exists.
+   *
+   * @throws Exception when things break
+   */
+  @Test
+  public void testSilentExistingWrite() throws Exception {
+    String path = methodDir.getAbsolutePath();
 
-    assertEquals("The next roll time should have been 990 ms in the future",
-        calendar.getTimeInMillis() + 990,
-        rfsSink.nextFlush.getTimeInMillis());
+    assertMetricsContents(doAppendTest(path, false, false, 2));
   }
 
   /**
-   * Test whether the roll interval is correctly calculated from the
-   * configuration settings.
+   * Test that writing fails when the directory isn't writable.
    */
   @Test
-  public void testGetRollInterval() {
-    doTestGetRollInterval(1, new String[] {"m", "min", "minute", "minutes"},
-        60 * 1000L);
-    doTestGetRollInterval(1, new String[] {"h", "hr", "hour", "hours"},
-        60 * 60 * 1000L);
-    doTestGetRollInterval(1, new String[] {"d", "day", "days"},
-        24 * 60 * 60 * 1000L);
+  public void testFailedWrite() {
+    String path = methodDir.getAbsolutePath();
+    MetricsSystem ms = initMetricsSystem(path, false, false);
 
-    ConfigBuilder builder = new ConfigBuilder();
-    SubsetConfiguration conf =
-        builder.add("sink.roll-interval", "1").subset("sink");
-    // We can reuse the same sink evry time because we're setting the same
-    // property every time.
-    RollingFileSystemSink sink = new RollingFileSystemSink();
+    new MyMetrics1().registerWith(ms);
 
-    sink.init(conf);
+    methodDir.setWritable(false);
+    MockSink.errored = false;
 
-    assertEquals(3600000L, sink.getRollInterval());
+    try {
+      // publish the metrics
+      ms.publishMetricsNow();
 
-    for (char c : "abcefgijklnopqrtuvwxyz".toCharArray()) {
-      builder = new ConfigBuilder();
-      conf = builder.add("sink.roll-interval", "90 " + c).subset("sink");
+      assertTrue("No exception was generated while writing metrics "
+          + "even though the target directory was not writable",
+          MockSink.errored);
 
-      try {
-        sink.init(conf);
-        sink.getRollInterval();
-        fail("Allowed flush interval with bad units: " + c);
-      } catch (MetricsException ex) {
-        // Expected
-      }
+      ms.stop();
+      ms.shutdown();
+    } finally {
+      // Make sure the dir is writable again so we can delete it at the end
+      methodDir.setWritable(true);
     }
   }
 
   /**
-   * Test the basic unit conversions with the given unit name modifier applied.
-   *
-   * @param mod a unit name modifier
+   * Test that writing fails silently when the directory is not writable.
    */
-  private void doTestGetRollInterval(int num, String[] units, long expected) {
-    RollingFileSystemSink sink = new RollingFileSystemSink();
-    ConfigBuilder builder = new ConfigBuilder();
+  @Test
+  public void testSilentFailedWrite() {
+    String path = methodDir.getAbsolutePath();
+    MetricsSystem ms = initMetricsSystem(path, true, false);
+
+    new MyMetrics1().registerWith(ms);
 
-    for (String unit : units) {
-      sink.init(builder.add("sink.roll-interval", num + unit).subset("sink"));
-      assertEquals(expected, sink.getRollInterval());
+    methodDir.setWritable(false);
+    MockSink.errored = false;
 
-      sink.init(builder.add("sink.roll-interval",
-          num + unit.toUpperCase()).subset("sink"));
-      assertEquals(expected, sink.getRollInterval());
+    try {
+      // publish the metrics
+      ms.publishMetricsNow();
 
-      sink.init(builder.add("sink.roll-interval",
-          num + " " + unit).subset("sink"));
-      assertEquals(expected, sink.getRollInterval());
+      assertFalse("An exception was generated while writing metrics "
+          + "when the target directory was not writable, even though the "
+          + "sink is set to ignore errors",
+          MockSink.errored);
 
-      sink.init(builder.add("sink.roll-interval",
-          num + " " + unit.toUpperCase()).subset("sink"));
-      assertEquals(expected, sink.getRollInterval());
+      ms.stop();
+      ms.shutdown();
+    } finally {
+      // Make sure the dir is writable again so we can delete it at the end
+      methodDir.setWritable(true);
     }
   }
 }
diff --git a/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/metrics2/sink/TestRollingFileSystemSinkWithLocal.java b/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/metrics2/sink/TestRollingFileSystemSinkWithLocal.java
deleted file mode 100644
index 96306bf..0000000
--- a/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/metrics2/sink/TestRollingFileSystemSinkWithLocal.java
+++ /dev/null
@@ -1,157 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.metrics2.sink;
-
-import org.apache.hadoop.metrics2.MetricsSystem;
-
-import org.junit.Test;
-import static org.junit.Assert.assertFalse;
-import static org.junit.Assert.assertTrue;
-
-/**
- * Test the {@link RollingFileSystemSink} class in the context of the local file
- * system.
- */
-public class TestRollingFileSystemSinkWithLocal
-    extends RollingFileSystemSinkTestBase {
-  /**
-   * Test writing logs to the local file system.
-   * @throws Exception when things break
-   */
-  @Test
-  public void testWrite() throws Exception {
-    String path = methodDir.getAbsolutePath();
-    MetricsSystem ms = initMetricsSystem(path, false, false);
-
-    assertMetricsContents(doWriteTest(ms, path, 1));
-  }
-
-  /**
-   * Test writing logs to the local file system with the sink set to ignore
-   * errors.
-   * @throws Exception when things break
-   */
-  @Test
-  public void testSilentWrite() throws Exception {
-    String path = methodDir.getAbsolutePath();
-    MetricsSystem ms = initMetricsSystem(path, true, false);
-
-    assertMetricsContents(doWriteTest(ms, path, 1));
-  }
-
-  /**
-   * Test writing logs to HDFS when the log file already exists.
-   *
-   * @throws Exception when things break
-   */
-  @Test
-  public void testExistingWrite() throws Exception {
-    String path = methodDir.getAbsolutePath();
-
-    assertMetricsContents(doAppendTest(path, false, false, 2));
-  }
-
-  /**
-   * Test writing logs to HDFS when the log file and the .1 log file already
-   * exist.
-   *
-   * @throws Exception when things break
-   */
-  @Test
-  public void testExistingWrite2() throws Exception {
-    String path = methodDir.getAbsolutePath();
-    MetricsSystem ms = initMetricsSystem(path, false, false);
-
-    preCreateLogFile(path, 2);
-
-    assertMetricsContents(doWriteTest(ms, path, 3));
-  }
-
-  /**
-   * Test writing logs to HDFS with ignore errors enabled when
-   * the log file already exists.
-   *
-   * @throws Exception when things break
-   */
-  @Test
-  public void testSilentExistingWrite() throws Exception {
-    String path = methodDir.getAbsolutePath();
-
-    assertMetricsContents(doAppendTest(path, false, false, 2));
-  }
-
-  /**
-   * Test that writing fails when the directory isn't writable.
-   */
-  @Test
-  public void testFailedWrite() {
-    String path = methodDir.getAbsolutePath();
-    MetricsSystem ms = initMetricsSystem(path, false, false);
-
-    new MyMetrics1().registerWith(ms);
-
-    methodDir.setWritable(false);
-    MockSink.errored = false;
-
-    try {
-      // publish the metrics
-      ms.publishMetricsNow();
-
-      assertTrue("No exception was generated while writing metrics "
-          + "even though the target directory was not writable",
-          MockSink.errored);
-
-      ms.stop();
-      ms.shutdown();
-    } finally {
-      // Make sure the dir is writable again so we can delete it at the end
-      methodDir.setWritable(true);
-    }
-  }
-
-  /**
-   * Test that writing fails silently when the directory is not writable.
-   */
-  @Test
-  public void testSilentFailedWrite() {
-    String path = methodDir.getAbsolutePath();
-    MetricsSystem ms = initMetricsSystem(path, true, false);
-
-    new MyMetrics1().registerWith(ms);
-
-    methodDir.setWritable(false);
-    MockSink.errored = false;
-
-    try {
-      // publish the metrics
-      ms.publishMetricsNow();
-
-      assertFalse("An exception was generated while writing metrics "
-          + "when the target directory was not writable, even though the "
-          + "sink is set to ignore errors",
-          MockSink.errored);
-
-      ms.stop();
-      ms.shutdown();
-    } finally {
-      // Make sure the dir is writable again so we can delete it at the end
-      methodDir.setWritable(true);
-    }
-  }
-}
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/metrics2/sink/TestRollingFileSystemSinkWithHdfs.java b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/metrics2/sink/TestRollingFileSystemSinkWithHdfs.java
index 9c80b2d..a4a2e71 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/metrics2/sink/TestRollingFileSystemSinkWithHdfs.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/metrics2/sink/TestRollingFileSystemSinkWithHdfs.java
@@ -20,6 +20,8 @@
 
 import java.io.IOException;
 import java.net.URI;
+import org.junit.After;
+import org.junit.Before;
 import java.util.Calendar;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.FileStatus;
@@ -29,8 +31,6 @@
 import org.apache.hadoop.metrics2.MetricsException;
 import org.apache.hadoop.metrics2.MetricsSystem;
 import org.apache.hadoop.metrics2.sink.RollingFileSystemSinkTestBase.MyMetrics1;
-import org.junit.After;
-import org.junit.Before;
 import org.junit.Test;
 import static org.junit.Assert.assertFalse;
 import static org.junit.Assert.assertTrue;
@@ -60,6 +60,7 @@ public void setupHdfs() throws IOException {
         new MiniDFSCluster.Builder(conf).numDataNodes(NUM_DATANODES).build();
 
     // Also clear sink flags
+    RollingFileSystemSink.flushQuickly = false;
     RollingFileSystemSink.hasFlushed = false;
   }
 
@@ -255,12 +256,10 @@ public void testSilentFailedClose() throws IOException {
    */
   @Test
   public void testFlushThread() throws Exception {
-    // Cause the sink's flush thread to be run immediately after the second
-    // metrics log is written
-    RollingFileSystemSink.forceFlush = true;
+    RollingFileSystemSink.flushQuickly = true;
 
     String path = "hdfs://" + cluster.getNameNode().getHostAndPort() + "/tmp";
-    MetricsSystem ms = initMetricsSystem(path, true, false, false);
+    MetricsSystem ms = initMetricsSystem(path, true, false);
 
     new MyMetrics1().registerWith(ms);
 
@@ -270,21 +269,14 @@ public void testFlushThread() throws Exception {
     // regardless.
     ms.publishMetricsNow();
 
-    int count = 0;
-
-    // Sleep until the flusher has run. This should never actually need to
-    // sleep, but the sleep is here to make sure this test isn't flakey.
+    // Sleep until the flusher has run
     while (!RollingFileSystemSink.hasFlushed) {
-      Thread.sleep(10L);
-
-      if (++count > 1000) {
-        fail("Flush thread did not run within 10 seconds");
-      }
+      Thread.sleep(50L);
     }
 
-    Calendar now = Calendar.getInstance();
-    Path currentDir = new Path(path, DATE_FORMAT.format(now.getTime()) + "00");
+    Calendar now = getNowNotTopOfHour();
     FileSystem fs = FileSystem.newInstance(new URI(path), new Configuration());
+    Path currentDir = new Path(path, DATE_FORMAT.format(now.getTime()));
     Path currentFile =
         findMostRecentLogFile(fs, new Path(currentDir, getLogFilename()));
     FileStatus status = fs.getFileStatus(currentFile);
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/metrics2/sink/TestRollingFileSystemSinkWithSecureHdfs.java b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/metrics2/sink/TestRollingFileSystemSinkWithSecureHdfs.java
index 8ef94fc..072cf41 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/metrics2/sink/TestRollingFileSystemSinkWithSecureHdfs.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/metrics2/sink/TestRollingFileSystemSinkWithSecureHdfs.java
@@ -49,11 +49,8 @@
 import org.apache.hadoop.security.SecurityUtil;
 import org.apache.hadoop.security.UserGroupInformation;
 import org.apache.hadoop.security.ssl.KeyStoreTestUtil;
-import org.junit.After;
-import org.junit.AfterClass;
 import org.junit.Test;
-import org.junit.Before;
-import org.junit.BeforeClass;
+import static org.junit.Assert.assertTrue;
 import static org.junit.Assert.assertTrue;
 
 /**
@@ -69,97 +66,56 @@
   private static String hdfsPrincipal;
   private static String hdfsKeytab;
   private static String spnegoPrincipal;
-  private MiniDFSCluster cluster = null;
-  private UserGroupInformation sink = null;
 
   /**
-   * Setup the KDC for testing a secure HDFS cluster.
+   * Do a basic write test against an HDFS cluster with Kerberos enabled. We
+   * assume that if a basic write succeeds, more complex operations will also
+   * succeed.
    *
-   * @throws Exception thrown if the KDC setup fails
+   * @throws Exception thrown if things break
    */
-  @BeforeClass
-  public static void initKdc() throws Exception {
-    Properties kdcConf = MiniKdc.createConf();
-    kdc = new MiniKdc(kdcConf, ROOT_TEST_DIR);
-    kdc.start();
+  @Test
+  public void testWithSecureHDFS() throws Exception {
+    RollingFileSystemSink.flushQuickly = false;
+    RollingFileSystemSink.hasFlushed = false;
+    initKdc();
 
-    File sinkKeytabFile = new File(ROOT_TEST_DIR, "sink.keytab");
-    sinkKeytab = sinkKeytabFile.getAbsolutePath();
-    kdc.createPrincipal(sinkKeytabFile, "sink/localhost");
-    sinkPrincipal = "sink/localhost@" + kdc.getRealm();
+    MiniDFSCluster cluster = null;
 
-    File hdfsKeytabFile = new File(ROOT_TEST_DIR, "hdfs.keytab");
-    hdfsKeytab = hdfsKeytabFile.getAbsolutePath();
-    kdc.createPrincipal(hdfsKeytabFile, "hdfs/localhost",
-        "HTTP/localhost");
-    hdfsPrincipal = "hdfs/localhost@" + kdc.getRealm();
-    spnegoPrincipal = "HTTP/localhost@" + kdc.getRealm();
-  }
+    try {
+      HdfsConfiguration conf = createSecureConfig("authentication,privacy");
 
-  /**
-   * Setup the mini-DFS cluster.
-   *
-   * @throws Exception thrown if the cluster setup fails
-   */
-  @Before
-  public void initCluster() throws Exception {
-    HdfsConfiguration conf = createSecureConfig("authentication,privacy");
+      RollingFileSystemSink.suppliedConf = conf;
 
-    RollingFileSystemSink.hasFlushed = false;
-    RollingFileSystemSink.suppliedConf = conf;
+      cluster = new MiniDFSCluster.Builder(conf).numDataNodes(NUM_DATANODES)
+          .build();
 
-    cluster = new MiniDFSCluster.Builder(conf).numDataNodes(NUM_DATANODES)
-        .build();
-    cluster.waitActive();
-    createDirectoriesSecurely();
-  }
+      cluster.waitActive();
 
-  /**
-   * Stop the mini-DFS cluster.
-   */
-  @After
-  public void stopCluster() {
-    if (cluster != null) {
-      cluster.shutdown();
-    }
+      UserGroupInformation sink = createDirectoriesSecurely(cluster);
+      final String path =
+          "hdfs://" + cluster.getNameNode().getHostAndPort() + "/tmp/test";
+      final MetricsSystem ms = initMetricsSystem(path, true, false, true);
 
-    // Restore non-secure conf
-    UserGroupInformation.setConfiguration(new Configuration());
-    RollingFileSystemSink.suppliedConf = null;
-    RollingFileSystemSink.suppliedFilesystem = null;
-  }
-
-  /**
-   * Stop the mini-KDC.
-   */
-  @AfterClass
-  public static void shutdownKdc() {
-    if (kdc != null) {
-      kdc.stop();
-    }
-  }
+      assertMetricsContents(
+          sink.doAs(new PrivilegedExceptionAction<String>() {
+            @Override
+            public String run() throws Exception {
+              return doWriteTest(ms, path, 1);
+            }
+          }));
+    } finally {
+      if (cluster != null) {
+        cluster.shutdown();
+      }
 
-  /**
-   * Do a basic write test against an HDFS cluster with Kerberos enabled. We
-   * assume that if a basic write succeeds, more complex operations will also
-   * succeed.
-   *
-   * @throws Exception thrown if things break
-   */
-  @Test
-  public void testWithSecureHDFS() throws Exception {
-    final String path =
-        "hdfs://" + cluster.getNameNode().getHostAndPort() + "/tmp/test";
-    final MetricsSystem ms =
-        initMetricsSystem(path, true, false, true);
+      shutdownKdc();
 
-    assertMetricsContents(
-        sink.doAs(new PrivilegedExceptionAction<String>() {
-          @Override
-          public String run() throws Exception {
-            return doWriteTest(ms, path, 1);
-          }
-        }));
+      // Restore non-secure conf
+      UserGroupInformation.setConfiguration(new Configuration());
+      RollingFileSystemSink.suppliedConf = null;
+      RollingFileSystemSink.suppliedFilesystem = null;
+    }
   }
 
   /**
@@ -170,25 +126,54 @@ public String run() throws Exception {
    */
   @Test
   public void testMissingPropertiesWithSecureHDFS() throws Exception {
-    final String path =
-        "hdfs://" + cluster.getNameNode().getHostAndPort() + "/tmp/test";
+    RollingFileSystemSink.flushQuickly = false;
+    RollingFileSystemSink.hasFlushed = false;
+    initKdc();
+
+    MiniDFSCluster cluster = null;
+
+    try {
+      HdfsConfiguration conf = createSecureConfig("authentication,privacy");
+
+      RollingFileSystemSink.suppliedConf = conf;
+
+      cluster = new MiniDFSCluster.Builder(conf).numDataNodes(NUM_DATANODES)
+          .build();
+
+      final String path =
+          "hdfs://" + cluster.getNameNode().getHostAndPort() + "/tmp/test";
 
-    initMetricsSystem(path, true, false);
+      createDirectoriesSecurely(cluster);
+      initMetricsSystem(path, true, false);
 
-    assertTrue("No exception was generated initializing the sink against a "
-        + "secure cluster even though the principal and keytab properties "
-        + "were missing", MockSink.errored);
+      assertTrue("No exception was generated initializing the sink against a "
+          + "secure cluster even though the principal and keytab properties "
+          + "were missing", MockSink.errored);
+    } finally {
+      if (cluster != null) {
+        cluster.shutdown();
+      }
+
+      shutdownKdc();
+
+      // Restore non-secure conf
+      UserGroupInformation.setConfiguration(new Configuration());
+      RollingFileSystemSink.suppliedConf = null;
+      RollingFileSystemSink.suppliedFilesystem = null;
+    }
   }
 
   /**
    * Create the /tmp directory as <i>hdfs</i> and /tmp/test as <i>sink</i> and
    * return the UGI for <i>sink</i>.
    *
+   * @param cluster the mini-cluster
+   * @return the UGI for <i>sink</i>
    * @throws IOException thrown if login or directory creation fails
    * @throws InterruptedException thrown if interrupted while creating a
    * file system handle
    */
-  protected void createDirectoriesSecurely()
+  protected UserGroupInformation createDirectoriesSecurely(final MiniDFSCluster cluster)
       throws IOException, InterruptedException {
     Path tmp = new Path("/tmp");
     Path test = new Path(tmp, "test");
@@ -207,9 +192,9 @@ public FileSystem run() throws Exception {
     fsForSuperUser.mkdirs(tmp);
     fsForSuperUser.setPermission(tmp, new FsPermission((short)0777));
 
-    sink = UserGroupInformation.loginUserFromKeytabAndReturnUGI(sinkPrincipal,
+    UserGroupInformation sink =
+        UserGroupInformation.loginUserFromKeytabAndReturnUGI(sinkPrincipal,
             sinkKeytab);
-
     FileSystem fsForSink =
         sink.doAs(new PrivilegedExceptionAction<FileSystem>() {
           @Override
@@ -220,6 +205,40 @@ public FileSystem run() throws Exception {
 
     fsForSink.mkdirs(test);
     RollingFileSystemSink.suppliedFilesystem = fsForSink;
+
+    return sink;
+  }
+
+  /**
+   * Setup the KDC for testing a secure HDFS cluster
+   *
+   * @throws Exception thrown if the KDC setup fails
+   */
+  public static void initKdc() throws Exception {
+    Properties kdcConf = MiniKdc.createConf();
+    kdc = new MiniKdc(kdcConf, methodDir);
+    kdc.start();
+
+    File sinkKeytabFile = new File(methodDir, "sink.keytab");
+    sinkKeytab = sinkKeytabFile.getAbsolutePath();
+    kdc.createPrincipal(sinkKeytabFile, "sink/localhost");
+    sinkPrincipal = "sink/localhost@" + kdc.getRealm();
+
+    File hdfsKeytabFile = new File(methodDir, "hdfs.keytab");
+    hdfsKeytab = hdfsKeytabFile.getAbsolutePath();
+    kdc.createPrincipal(hdfsKeytabFile, "hdfs/localhost",
+        "HTTP/localhost");
+    hdfsPrincipal = "hdfs/localhost@" + kdc.getRealm();
+    spnegoPrincipal = "HTTP/localhost@" + kdc.getRealm();
+  }
+
+  /**
+   * Stop the mini-KDC.
+   */
+  public static void shutdownKdc() {
+    if (kdc != null) {
+      kdc.stop();
+    }
   }
 
   /**
-- 
1.7.9.5

